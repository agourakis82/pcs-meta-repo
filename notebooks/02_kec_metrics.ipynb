{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240fb2e8",
   "metadata": {},
   "source": [
    "# PCS‑HELIO v4.3 — 02 · KEC Metrics\n",
    "Compute Knowledge Entropy Curvature (KEC) metrics from SWOW; write standardized outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helio-preflight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from notebooks._fragments import apply_style, preflight_checks, print_contract\n",
    "apply_style(); preflight_checks(); print_contract()\n",
    "RUN_MODE = os.environ.get('RUN_MODE','sample')\n",
    "BASE=Path('.') ; DATA=BASE/'data' ; PROC=DATA/'processed' ; RPTS=BASE/'reports'\n",
    "(PROC/'kec').mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd290265",
   "metadata": {},
   "source": [
    "# Notebook 02: KEC Metrics Computation\n",
    "Using the SWOW graph from Notebook 01, this notebook computes the *Knowledge Entropy Curvature* (KEC) metrics for each node/word:\n",
    "- **Transition Entropy:** The entropy of outgoing edge weight distribution (uncertainty of associations).\n",
    "- **Local Curvature:** Graph curvature at the node (using Ollivier-Ricci or Forman's method).\n",
    "- **Meso-scale Coherence:** Community-based coherence (e.g., modularity or cluster tightness around the node).\n",
    "\n",
    "Ablation experiments (e.g., edge weight shuffling) and uncertainty estimation (bootstrap confidence intervals) are included.\n",
    "- **Input:** Graph from Notebook 01 (or edge list).\n",
    "- **Output:** Table of KEC metrics per word (saved to `data/processed/kec/metrics_{LANG}.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SWOW graph from Notebook 01\n",
    "import pickle\n",
    "LANG = 'en'\n",
    "graph_file = str((PROC / f'swow_graph_{LANG}.pkl'))\n",
    "with open(graph_file, 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "print(f\"Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "# Create undirected version for community detection\n",
    "G_ud = G.to_undirected()\n",
    "# In sample mode, restrict to a subgraph of top-degree nodes for speed\n",
    "if 'RUN_MODE' in globals() and RUN_MODE == 'sample' and G.number_of_nodes() > 1000:\n",
    "    deg = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
    "    keep = [n for n,_ in deg[:2000]]\n",
    "    G = G.subgraph(keep).copy()\n",
    "    G_ud = G_ud.subgraph(keep).copy()\n",
    "print(f\"Undirected graph created: {G_ud.number_of_nodes()} nodes, {G_ud.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "967de366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated entropy for 166540 nodes.\n",
      "Saved entropy to ../data/processed/kec/entropy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Assume G (graph) is available (from Notebook 01)\n",
    "import math\n",
    "\n",
    "# Compute transition entropy for each node\n",
    "entropy = {}\n",
    "for node in G.nodes():\n",
    "    out_edges = G.out_edges(node, data='weight')\n",
    "    total_w = sum([w for _,_,w in out_edges])\n",
    "    H = 0.0\n",
    "    for _, target, w in out_edges:\n",
    "        p = w / total_w\n",
    "        if p > 0:\n",
    "            H -= p * math.log2(p)\n",
    "    entropy[node] = H\n",
    "print(f\"Calculated entropy for {len(entropy)} nodes.\")\n",
    "\n",
    "# Save entropy as parquet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "OUTDIR = PROC/\"kec\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "ent_df = pd.DataFrame(list(entropy.items()), columns=['node', 'entropy'])\n",
    "ent_df.to_parquet(OUTDIR/\"entropy.parquet\")\n",
    "print(f\"Saved entropy to {OUTDIR/'entropy.parquet'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8474405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed curvature for 1537892 edges (sample edge curvatures shown below):\n",
      "[(('there', 'position'), 764), (('there', 'place'), 1151), (('there', 'point'), 975), (('there', 'over'), 1049), (('there', 'here'), 714)]\n",
      "Saved curvature to ../data/processed/kec/curvature.parquet\n",
      "Saved curvature to ../data/processed/kec/curvature.parquet\n"
     ]
    }
   ],
   "source": [
    "# Compute local curvature (using a placeholder or external library)\n",
    "try:\n",
    "    import GraphRicciCurvature\n",
    "    # Using Ollivier-Ricci from an external lib if available\n",
    "    orc = GraphRicciCurvature.OllivierRicci(G, alpha=0.5, verbose=False)\n",
    "    orc.compute_ricci_curvature()\n",
    "    curvature = {edge: data['ricciCurvature'] for edge, data in orc.G.edges.items()}\n",
    "except ImportError:\n",
    "    # Placeholder: approximate curvature by Forman's method as fallback\n",
    "    curvature = {}\n",
    "    for u,v in G.edges():\n",
    "        curvature[(u,v)] = (G.degree(u) + G.degree(v) - 2)  # simplistic Forman proxy\n",
    "print(f\"Computed curvature for {len(curvature)} edges (sample edge curvatures shown below):\")\n",
    "print(list(curvature.items())[:5])\n",
    "\n",
    "# Save curvature as parquet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "OUTDIR = PROC/\"kec\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "curv_df = pd.DataFrame(list(curvature.items()), columns=['edge', 'curvature'])\n",
    "curv_df.to_parquet(OUTDIR/\"curvature.parquet\")\n",
    "print(f\"Saved curvature to {OUTDIR/'curvature.parquet'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65e23cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Leiden algorithm for community detection\n",
      "Computed coherence for 166540 nodes.\n",
      "Computed coherence for 166540 nodes.\n",
      "Saved communities to ../data/processed/kec/communities_leiden.parquet\n",
      "Saved coherence to ../data/processed/kec/coherence.parquet\n",
      "Saved communities to ../data/processed/kec/communities_leiden.parquet\n",
      "Saved coherence to ../data/processed/kec/coherence.parquet\n"
     ]
    }
   ],
   "source": [
    "# Compute coherence: use community detection (e.g., Leiden) to get cluster assignments and measure cluster purity around node\n",
    "# Note: greedy_modularity_communities can be slow on large graphs, consider using leidenalg if available\n",
    "try:\n",
    "    import leidenalg\n",
    "    import igraph as ig\n",
    "    # Convert NetworkX graph to igraph for faster community detection\n",
    "    nx_g = G_ud  # Use undirected graph\n",
    "    ig_g = ig.Graph.from_networkx(nx_g)\n",
    "    partition = leidenalg.find_partition(ig_g, leidenalg.ModularityVertexPartition)\n",
    "    communities = [set(ig_g.vs[comm]['_nx_name'] for comm in part) for part in partition]\n",
    "    print(\"Used Leiden algorithm for community detection\")\n",
    "except ImportError:\n",
    "    print(\"leidenalg not available, using NetworkX greedy_modularity_communities (may be slow)\")\n",
    "    import networkx.algorithms.community as nx_comm\n",
    "    communities = nx_comm.greedy_modularity_communities(G_ud)  # Use undirected\n",
    "\n",
    "node_to_comm = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for node in comm:\n",
    "        node_to_comm[node] = i\n",
    "\n",
    "coherence = {}\n",
    "for node in G_ud.nodes():  # Use undirected for consistency\n",
    "    # e.g., coherence = fraction of node's neighbors in same community\n",
    "    neighbors = list(G_ud.neighbors(node))\n",
    "    if neighbors:\n",
    "        same_comm = sum(1 for n in neighbors if node_to_comm.get(n) == node_to_comm.get(node))\n",
    "        coherence[node] = same_comm / len(neighbors)\n",
    "    else:\n",
    "        coherence[node] = None\n",
    "print(f\"Computed coherence for {len(coherence)} nodes.\")\n",
    "\n",
    "# Save communities and coherence as parquet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "OUTDIR = PROC/\"kec\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "comm_df = pd.DataFrame(list(node_to_comm.items()), columns=['node', 'community'])\n",
    "comm_df.to_parquet(OUTDIR/\"communities_leiden.parquet\")\n",
    "coh_df = pd.DataFrame(list(coherence.items()), columns=['node', 'coherence'])\n",
    "coh_df.to_parquet(OUTDIR/\"coherence.parquet\")\n",
    "print(f\"Saved communities to {OUTDIR/'communities_leiden.parquet'}\")\n",
    "print(f\"Saved coherence to {OUTDIR/'coherence.parquet'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05176ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KEC metrics for 166540 nodes to /home/agourakis82/workspace/pcs-meta-repo/notebooks/../data/processed/kec/metrics_en.csv\n",
      "Sample metrics:\n",
      "       node   entropy  degree  coherence  avg_curvature\n",
      "0     there  5.481249     326   0.533333     708.580357\n",
      "1  position  6.685050     440   0.451948     909.690217\n",
      "2     place  6.138368     827   0.165354    1411.185714\n",
      "3     point  6.705326     651   0.449324    1170.403409\n",
      "4      true  5.858416     506   0.493363     992.325397\n",
      "Saved KEC metrics as parquet to /home/agourakis82/workspace/pcs-meta-repo/notebooks/../data/processed/kec/metrics_en.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save the computed metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = str(PROC / 'kec')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prepare data for saving\n",
    "metrics_data = []\n",
    "for node in G.nodes():\n",
    "    row = {\n",
    "        'node': node,\n",
    "        'entropy': entropy.get(node, None),\n",
    "        'degree': G.degree(node),\n",
    "        'coherence': coherence.get(node, None) if 'coherence' in locals() else None\n",
    "    }\n",
    "    # Add curvature for edges connected to this node (average or max)\n",
    "    node_curvatures = [curvature.get((node, neighbor), 0) for neighbor in G.neighbors(node)]\n",
    "    row['avg_curvature'] = sum(node_curvatures) / len(node_curvatures) if node_curvatures else 0\n",
    "    metrics_data.append(row)\n",
    "\n",
    "# Save to CSV\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "output_file = os.path.join(output_dir, f'metrics_{LANG}.csv')\n",
    "metrics_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved KEC metrics for {len(metrics_data)} nodes to {output_file}\")\n",
    "print(f\"Sample metrics:\\n{metrics_df.head()}\")\n",
    "\n",
    "# Also save as parquet for QA\n",
    "from pathlib import Path\n",
    "OUTDIR = Path(output_dir)\n",
    "metrics_df.to_parquet(OUTDIR/f\"metrics_{LANG}.parquet\")\n",
    "print(f\"Saved KEC metrics as parquet to {OUTDIR/f'metrics_{LANG}.parquet'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b7195b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checagem de alinhamento entre tabelas\n",
    "n_nodes = len(set(ent_df[\"node\"]) | set(comm_df[\"node\"]) | set(coh_df[\"node\"]))\n",
    "coverage_kec = len(kec_df) / n_nodes if n_nodes else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "019604ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_607762/2478482694.py:21: DeprecationWarning: Graph.clusters() is deprecated; use Graph.connected_components() instead\n",
      "  giant = g.clusters().giant()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ coherence_null.parquet salvo\n",
      "Δcoherence (real - nulo): {'count': 166540.0, 'mean': 0.0521, 'std': 0.1549, 'min': -0.8333, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 0.9259}\n",
      "Δcoherence (real - nulo): {'count': 166540.0, 'mean': 0.0521, 'std': 0.1549, 'min': -0.8333, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 0.9259}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx, random, pandas as pd\n",
    "from pathlib import Path\n",
    "import igraph as ig, leidenalg as la\n",
    "\n",
    "random.seed(42)\n",
    "# Copia o grafo não-direcionado já podado\n",
    "assert 'G_ud' in globals(), \"G_ud indisponível — rode as células anteriores.\"\n",
    "G_null = G_ud.copy()\n",
    "\n",
    "# Grau-preservado (double-edge-swap) — cuidado com custo; rode com 'nswap' proporcional às arestas\n",
    "nswap = min(5 * G_null.number_of_edges(), 200000)  # limite de segurança\n",
    "try:\n",
    "    G_null = nx.double_edge_swap(G_null, nswap=nswap, max_tries=nswap*5, seed=42)\n",
    "except Exception as e:\n",
    "    print(\"Aviso: swap parcial:\", e)\n",
    "\n",
    "# Leiden em CPU para o nulo (poderia ser GPU, se quiser)\n",
    "edges = [(u, v, G_null[u][v].get('weight',1.0)) for u, v in G_null.edges()]\n",
    "g = ig.Graph.TupleList(edges, directed=False, edge_attrs=[\"weight\"])\n",
    "g.simplify(combine_edges={\"weight\":\"sum\"})\n",
    "giant = g.clusters().giant()\n",
    "part = la.find_partition(giant, la.RBConfigurationVertexPartition, \n",
    "                         weights=\"weight\", resolution_parameter=1.0, seed=42)\n",
    "\n",
    "node_ids = giant.vs[\"name\"]\n",
    "membership = part.membership\n",
    "node_to_comm_null = dict(zip(node_ids, membership))\n",
    "\n",
    "# Coerência no nulo (mesma função do caminho CPU, sem amostragem por simplicidade)\n",
    "rows = []\n",
    "for node in G_null.nodes():\n",
    "    neigh = list(G_null.neighbors(node))\n",
    "    if not neigh: \n",
    "        coh = None\n",
    "    else:\n",
    "        same = sum(1 for n in neigh if node_to_comm_null.get(n) == node_to_comm_null.get(node))\n",
    "        coh = same/len(neigh)\n",
    "    rows.append((node, coh))\n",
    "\n",
    "coh_null = pd.DataFrame(rows, columns=[\"node\",\"coherence_null\"])\n",
    "coh_null.to_parquet(str(PROC/ 'kec' / 'coherence_null.parquet'), index=False)\n",
    "print(\"✓ coherence_null.parquet salvo\")\n",
    "\n",
    "# Pequena comparação resumida\n",
    "coh_df = pd.read_parquet(str(PROC/ 'kec' / 'coherence.parquet'))\n",
    "cmp_df = coh_df.merge(coh_null, on=\"node\", how=\"inner\").dropna()\n",
    "delta = (cmp_df[\"coherence\"] - cmp_df[\"coherence_null\"]).describe().to_dict()\n",
    "print(\"Δcoherence (real - nulo):\", {k: round(v,4) if isinstance(v,(int,float)) else v for k,v in delta.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e90f9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Rewrote ../data/processed/kec/metrics_en.csv with columns: ['token_norm', 'entropy', 'curvature', 'coherence']\n"
     ]
    }
   ],
   "source": [
    "# Finalize KEC metrics: ensure curvature alias and token_norm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "kec_path = Path(str(PROC/ 'kec' / 'metrics_en.csv'))\n",
    "if kec_path.exists():\n",
    "    df = pd.read_csv(kec_path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # curvature alias\n",
    "    if 'avg_curvature' in df.columns and 'curvature' not in df.columns:\n",
    "        df = df.rename(columns={'avg_curvature':'curvature'})\n",
    "    # token_norm\n",
    "    def norm_token(s):\n",
    "        if not isinstance(s,str): return s\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'[\\W_]+','',s)\n",
    "        return s\n",
    "    src = 'node' if 'node' in df.columns else ('word' if 'word' in df.columns else None)\n",
    "    if src and 'token_norm' not in df.columns:\n",
    "        df['token_norm'] = df[src].astype(str).map(norm_token)\n",
    "    # minimal required cols\n",
    "    need = [c for c in ['token_norm','entropy','curvature','coherence'] if c in df.columns]\n",
    "    if need:\n",
    "        df.to_csv(kec_path, index=False)\n",
    "        print(f\"[OK] Rewrote {kec_path} with columns: {need}\")\n",
    "    else:\n",
    "        print(\"[WARN] Required KEC columns missing; file left unchanged.\")\n",
    "else:\n",
    "    print(f\"[WARN] Missing {kec_path}; run Notebook 02 cells above to generate it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9124abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v4.3] Rewrote metrics_en.csv with columns ['token_norm', 'entropy', 'curvature', 'coherence']\n"
     ]
    }
   ],
   "source": [
    "# v4.3 finalization: enforce KEC output schema\n",
    "from pathlib import Path\n",
    "import pandas as pd, re\n",
    "PROC=Path('data/processed'); KEC=PROC/'kec'; KEC.mkdir(parents=True, exist_ok=True)\n",
    "path=KEC/'metrics_en.csv'\n",
    "if path.exists():\n",
    "    df=pd.read_csv(path)\n",
    "    if 'curvature' not in df.columns and 'avg_curvature' in df.columns:\n",
    "        df=df.rename(columns={'avg_curvature':'curvature'})\n",
    "    if 'token_norm' not in df.columns:\n",
    "        def norm_token(s):\n",
    "            if not isinstance(s,str): return s\n",
    "            s=s.lower(); s=re.sub(r'[\\W_]+','',s); return s\n",
    "        src='token_norm' if 'token_norm' in df.columns else ('node' if 'node' in df.columns else ('word' if 'word' in df.columns else ('Word' if 'Word' in df.columns else None)))\n",
    "        if src:\n",
    "            df['token_norm']=df[src].astype(str).map(norm_token)\n",
    "    need=['token_norm','entropy','curvature','coherence']\n",
    "    for c in need:\n",
    "        if c not in df.columns:\n",
    "            df[c]=pd.NA\n",
    "    df=df[need]\n",
    "    df.to_csv(path, index=False)\n",
    "    print('[v4.3] Rewrote metrics_en.csv with columns', need)\n",
    "else:\n",
    "    print('[v4.3] KEC file not found; skip schema finalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcs-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
