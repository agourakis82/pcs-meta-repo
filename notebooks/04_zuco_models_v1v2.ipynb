{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "version": "3.x"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Notebook 04 \u2014 ZuCo Models (v1+v2): KEC \u00d7 Dataset/Task (v4.3, Upgraded)\n", "\n", "OLS (clustered SE), FDR, MixedLM (Subject + vc SentenceID), Bootstrap (joblib), partial plots & EEG trends.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["import os, time, warnings, re\n", "from pathlib import Path\n", "import numpy as np, pandas as pd\n", "import statsmodels.api as sm\n", "import statsmodels.formula.api as smf\n", "import matplotlib.pyplot as plt\n", "from statsmodels.stats.multitest import multipletests\n", "SEED=42; np.random.seed(SEED)\n", "FIG_DIR=Path('figures/metrics'); FIG_DIR.mkdir(parents=True, exist_ok=True)\n", "PROC_DIR=Path('data/processed'); PROC_DIR.mkdir(parents=True, exist_ok=True)\n", "RPT_DIR=Path('reports'); RPT_DIR.mkdir(parents=True, exist_ok=True)\n", "def heartbeat(m): print(f\"[{time.strftime('%H:%M:%S')}] {m}\")\n", "def norm_token(s):\n", "    if not isinstance(s,str): return s\n", "    s=s.lower(); s=re.sub(r'[\\W_]+','',s); return s\n", "heartbeat('Env ready')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["zuco=pd.read_csv(PROC_DIR/'zuco_aligned.csv') if (PROC_DIR/'zuco_aligned.csv').exists() else pd.DataFrame()\n", "kec=pd.read_csv(PROC_DIR/'kec'/'metrics_en.csv') if (PROC_DIR/'kec'/'metrics_en.csv').exists() else pd.DataFrame()\n", "for d in (zuco,kec):\n", "    if len(d)>0: d.columns=[c.strip() for c in d.columns]\n", "if len(zuco)>0: zuco['token_norm']=zuco.get('Word', zuco.get('word','')).astype(str).map(norm_token)\n", "if len(kec)>0:\n", "    src_word='word' if 'word' in kec.columns else ('Word' if 'Word' in kec.columns else None)\n", "    if src_word: kec['token_norm']=kec[src_word].astype(str).map(norm_token)\n", "merged = zuco.merge(kec[['token_norm','entropy','curvature','coherence']] if 'token_norm' in kec.columns else pd.DataFrame(),\n", "                    on='token_norm', how='left', suffixes=('','_kec')) if len(zuco)>0 and len(kec)>0 else pd.DataFrame()\n", "heartbeat(f'Merged shape: {merged.shape if len(merged)>0 else \"N/A\"}')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["df=merged.copy()\n", "if len(df)>0:\n", "    df=df.rename(columns={'LogFreq':'log_freq'})\n", "    for resp in ('TRT','GPT'):\n", "        if resp in df: df[f'log_{resp}']=np.log1p(df[resp])\n", "    for cat in ('Dataset','Task','Subject','SentenceID'):\n", "        if cat in df: df[cat]=df[cat].astype(str)\n", "    ET_COLS=[c for c in ('FFD','GD','log_TRT','log_GPT') if c in df.columns]\n", "    KEC_COLS=[c for c in ('entropy','curvature','coherence') if c in df.columns]\n", "    COVS=[c for c in ('length','log_freq','surprisal') if c in df.columns]\n", "    heartbeat(f'ET:{ET_COLS} KEC:{KEC_COLS} COVS:{COVS}')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["reading_results=[]\n", "if len(df)>0 and 'Subject' in df.columns:\n", "    for resp in [r for r in ('FFD','GD','log_TRT','log_GPT') if r in df.columns]:\n", "        preds=[c for c in ['entropy','curvature','coherence'] if c in df.columns]\n", "        preds+=[c for c in ['length','log_freq','surprisal'] if c in df.columns]\n", "        preds+=[c for c in ['Dataset','Task'] if c in df.columns]\n", "        if 'entropy' in df.columns and 'Task' in df.columns: preds.append('entropy:Task')\n", "        if len(preds)<3: heartbeat(f'Skip {resp}'); continue\n", "        formula=f\"{resp} ~ \"+' + '.join(preds)\n", "        heartbeat(f'OLS: {formula}')\n", "        try:\n", "            m=smf.ols(formula, data=df).fit(); rob=m.get_robustcov_results(cov_type='cluster', groups=df['Subject'])\n", "            coefs=rob.params.rename('coef').to_frame(); coefs['se']=rob.bse; coefs['t']=rob.tvalues; coefs['p']=rob.pvalues; coefs['response']=resp\n", "            reading_results.append(coefs.reset_index().rename(columns={'index':'term'}))\n", "        except Exception as e:\n", "            warnings.warn(f'OLS failed for {resp}: {e}')\n", "if reading_results:\n", "    tbl=pd.concat(reading_results, ignore_index=True); (PROC_DIR/'models_reading_coeffs.csv').write_text(tbl.to_csv(index=False))\n", "else:\n", "    heartbeat('No OLS results')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["inp=PROC_DIR/'models_reading_coeffs.csv'\n", "if inp.exists():\n", "    dfc=pd.read_csv(inp); outs=[]\n", "    for resp,grp in dfc.groupby('response'):\n", "        p=grp['p'].values; rej,q,_,_=multipletests(p, alpha=0.05, method='fdr_bh')\n", "        g=grp.copy(); g['p_fdr_bh']=q; g['rej_fdr_bh_0.05']=rej; outs.append(g)\n", "    fdr=pd.concat(outs, ignore_index=True); (PROC_DIR/'models_reading_coeffs_fdr.csv').write_text(fdr.to_csv(index=False))\n", "    print('Saved FDR')\n", "else:\n", "    print('Skip FDR')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["def partial_plot(dfin, response='FFD', xvar='entropy', controls=None, fixed_cats=None, n_points=100, savepath=None):\n", "    controls = controls or []; fixed_cats = fixed_cats or {}\n", "    need=[response,xvar]+controls+list(fixed_cats.keys())\n", "    if not set(need).issubset(dfin.columns): return\n", "    formula=response+' ~ '+xvar\n", "    if controls: formula+=' + '+' + '.join(controls)\n", "    for k in fixed_cats:\n", "        if k in dfin.columns and dfin[k].dtype=='O': formula+=f' + C({k})'\n", "    mod=smf.ols(formula, data=dfin.dropna(subset=need)).fit()\n", "    xgrid=np.linspace(dfin[xvar].quantile(0.05), dfin[xvar].quantile(0.95), n_points)\n", "    base={c: float(dfin[c].median()) for c in controls}; base.update(fixed_cats)\n", "    grid=pd.DataFrame({xvar:xgrid, **base}); yhat=mod.predict(grid)\n", "    plt.figure(); samp=dfin.dropna(subset=need).sample(min(2000,len(dfin.dropna(subset=need))), random_state=SEED)\n", "    plt.scatter(samp[xvar], samp[response], alpha=0.2); plt.plot(xgrid, yhat)\n", "    ttl=', '.join([f\"{k}={v}\" for k,v in fixed_cats.items()]) if fixed_cats else 'overall'\n", "    plt.title(f'{response} ~ {xvar} | {ttl}'); plt.xlabel(xvar); plt.ylabel(response); plt.tight_layout()\n", "    if savepath: plt.savefig(savepath, dpi=150)\n", "    plt.show()\n", "if len(df)>0 and {'FFD','entropy'}.issubset(df.columns):\n", "    controls=[c for c in ['length','log_freq'] if c in df.columns]\n", "    partial_plot(df,'FFD','entropy',controls,{},savepath=Path('figures/metrics')/'F2_reading_vs_KEC.png')\n", "    if 'Dataset' in df:\n", "        for ds in sorted(df['Dataset'].dropna().unique()):\n", "            dsd=df[df['Dataset']==ds]\n", "            partial_plot(dsd,'FFD','entropy',controls,{'Dataset':ds},savepath=Path('figures/metrics')/f'F2_reading_vs_KEC_{ds}.png')\n", "    if 'Task' in df:\n", "        for tk in sorted(df['Task'].dropna().unique()):\n", "            tkd=df[df['Task']==tk]\n", "            partial_plot(tkd,'FFD','entropy',controls,{'Task':tk},savepath=Path('figures/metrics')/f'F2_reading_vs_KEC_task_{tk}.png')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["def per_subject_trends(dfin, eeg_col='ThetaPower', xvar='entropy', by='Dataset', saveprefix='F3_EEG_vs_KEC'):\n", "    import math\n", "    if not {'Subject',eeg_col,xvar}.issubset(dfin.columns): return\n", "    groups=sorted(dfin[by].dropna().unique()) if (by and by in dfin.columns) else [None]\n", "    for g in groups:\n", "        dsub=dfin if g is None else dfin[dfin[by]==g]\n", "        subs=sorted(dsub['Subject'].astype(str).unique()); nsub=len(subs)\n", "        if nsub==0: continue\n", "        ncols=4; nrows=int(math.ceil(nsub/ncols))\n", "        plt.figure(figsize=(12,3*nrows))\n", "        for i,s in enumerate(subs,1):\n", "            ax=plt.subplot(nrows,ncols,i)\n", "            sdf=dsub[dsub['Subject'].astype(str)==str(s)].dropna(subset=[eeg_col,xvar])\n", "            if len(sdf)<5: ax.set_title(f'{s} (few)'); continue\n", "            ax.scatter(sdf[xvar], sdf[eeg_col], alpha=0.3)\n", "            try:\n", "                m=smf.ols(f\"{eeg_col} ~ {xvar}\", data=sdf).fit(); xs=np.linspace(sdf[xvar].min(), sdf[xvar].max(), 50)\n", "                ys=m.params['Intercept'] + m.params[xvar]*xs; ax.plot(xs, ys)\n", "            except Exception: pass\n", "            ax.set_title(str(s)); ax.set_xlabel(xvar); ax.set_ylabel(eeg_col)\n", "        plt.tight_layout(); tag=f'_{by}_{g}' if g is not None else ''\n", "        out=Path('figures/metrics')/f'{saveprefix}{tag}.png'; plt.savefig(out, dpi=150)\n", "        plt.show()\n", "if len(df)>0 and 'entropy' in df.columns and any(c in df.columns for c in ['ThetaPower','AlphaPower']):\n", "    eegc='ThetaPower' if 'ThetaPower' in df.columns else 'AlphaPower'\n", "    per_subject_trends(df, eeg_col=eegc, xvar='entropy', by='Dataset', saveprefix='F3_EEG_vs_KEC')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["from statsmodels.regression.mixed_linear_model import MixedLM\n", "def fit_mixedlm_ffd(dfin):\n", "    req={'FFD','entropy','length','log_freq','Subject','SentenceID'}\n", "    if not req.issubset(dfin.columns): return None\n", "    use=['FFD','entropy','length','log_freq','Subject','SentenceID']\n", "    if 'Task' in dfin.columns: use.append('Task')\n", "    if 'Dataset' in dfin.columns: use.append('Dataset')\n", "    mdf=dfin[use].dropna().copy()\n", "    # dummies\n", "    import pandas as pd\n", "    for cat in ['Task','Dataset']:\n", "        if cat in mdf.columns:\n", "            dummies=pd.get_dummies(mdf[cat], prefix=cat, drop_first=True)\n", "            mdf=pd.concat([mdf.drop(columns=[cat]), dummies], axis=1)\n", "    exog_cols=[c for c in mdf.columns if c not in ['FFD','Subject','SentenceID']]\n", "    exog=sm.add_constant(mdf[exog_cols]); endog=mdf['FFD']\n", "    vc={'Sentence': '0 + C(SentenceID)'}\n", "    try:\n", "        model=MixedLM(endog, exog, groups=mdf['Subject'], vc_formula=vc)\n", "        res=model.fit(reml=True, method='lbfgs')\n", "        Path('data/processed')/'mixedlm_ffd_summary.txt'\n", "        (Path('data/processed')/'mixedlm_ffd_summary.txt').write_text(str(res.summary()))\n", "        print('Saved MixedLM summary')\n", "        return res\n", "    except Exception as e:\n", "        warnings.warn(f'MixedLM failed: {e}'); return None\n", "# Uncomment to run: _=fit_mixedlm_ffd(df)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["from joblib import Parallel, delayed\n", "def _boot_fit_once(groups, dfin, formula, rng):\n", "    samp_groups=rng.choice(groups, size=len(groups), replace=True)\n", "    import pandas as pd\n", "    samp_df=pd.concat([dfin[dfin['Subject']==g] for g in samp_groups], ignore_index=True)\n", "    try:\n", "        m=smf.ols(formula, data=samp_df).fit(); return m.params\n", "    except Exception: return None\n", "def bootstrap_coefs_parallel(dfin, formula, group_col='Subject', B=1000, n_jobs=-1, seed=42):\n", "    rng=np.random.default_rng(seed); groups=dfin[group_col].unique()\n", "    seeds=rng.integers(0, 2**32-1, size=B, endpoint=True)\n", "    res=Parallel(n_jobs=n_jobs, verbose=10)(delayed(_boot_fit_once)(groups, dfin, formula, np.random.default_rng(int(s))) for s in seeds)\n", "    import pandas as pd\n", "    return pd.DataFrame([r for r in res if r is not None])\n", "# Example:\n", "# if len(df)>0 and {'FFD','entropy','length','log_freq','Subject'}.issubset(df.columns):\n", "#     boot=bootstrap_coefs_parallel(df[['FFD','entropy','length','log_freq','Subject']].dropna(),'FFD ~ entropy + length + log_freq',B=1000,n_jobs=-1)\n", "#     boot.to_csv(Path('data/processed')/'boot_ols_ffd_entropy.csv', index=False)\n", "#     print('Saved bootstrap')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["import json\n", "qa={'timestamp_utc':'2025-09-01T19:48:51.873406Z',\n", "    'merged_rows': int(len(df)) if len(df)>0 else 0,\n", "    'et_cols': [c for c in ('FFD','GD','log_TRT','log_GPT') if c in (df.columns if len(df)>0 else [])],\n", "    'kec_cols':[c for c in ('entropy','curvature','coherence') if c in (df.columns if len(df)>0 else [])],\n", "    'cat_covs':[c for c in ('Dataset','Task') if c in (df.columns if len(df)>0 else [])],\n", "    'reading_coeffs_csv': 'data/processed/models_reading_coeffs.csv' if (Path('data/processed')/'models_reading_coeffs.csv').exists() else None,\n", "    'reading_coeffs_fdr_csv': 'data/processed/models_reading_coeffs_fdr.csv' if (Path('data/processed')/'models_reading_coeffs_fdr.csv').exists() else None,\n", "    'mixedlm_summary': 'data/processed/mixedlm_ffd_summary.txt' if (Path('data/processed')/'mixedlm_ffd_summary.txt').exists() else None,\n", "    'fig_f2': [str(p) for p in Path('figures/metrics').glob('F2_reading_vs_KEC*.png')],\n", "    'fig_f3': [str(p) for p in Path('figures/metrics').glob('F3_EEG_vs_KEC*.png')]\n", "}\n", "(Path('reports')/'qa_kec_models.json').write_text(json.dumps(qa, indent=2))\n", "print('Saved QA')\n"]}]}