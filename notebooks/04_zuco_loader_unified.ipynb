{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170156e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2861286127.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**Objective:** This notebook loads, processes, and unifies word-level eye-tracking (ET) and EEG data from the ZuCo v1 and v2 datasets. It is designed to create a clean, modeling-ready dataset that harmonizes the different data structures of the two versions.\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ZuCo v1 &amp; v2 Unified Data Loader\n",
    "\n",
    "**Objective:** This notebook loads, processes, and unifies word-level eye-tracking (ET) and EEG data from the ZuCo v1 and v2 datasets. It is designed to create a clean, modeling-ready dataset that harmonizes the different data structures of the two versions.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 1. Setup and Configuration\n",
    "Import necessary libraries (pandas, numpy, scipy, h5py, joblib). Define global paths for raw data input and processed data output. Include helper functions for logging and token normalization.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "import os, time, warnings, re, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Dependency checks\n",
    "try:\n",
    "    from scipy.io import loadmat\n",
    "    _HAS_SCIPY = True\n",
    "except ImportError:\n",
    "    _HAS_SCIPY = False\n",
    "    warnings.warn(\"scipy not found, .mat file processing will be skipped.\")\n",
    "\n",
    "try:\n",
    "    import h5py\n",
    "    _HAS_H5PY = True\n",
    "except ImportError:\n",
    "    _HAS_H5PY = False\n",
    "    warnings.warn(\"h5py not found, v7.3 .mat file processing will be skipped.\")\n",
    "\n",
    "# --- Global paths and configuration ---\n",
    "BASE_DIR = Path.cwd()\n",
    "RAW_V1 = BASE_DIR.parent / 'data/raw_public/zuco/v1'\n",
    "RAW_V2 = BASE_DIR.parent / 'data/raw_public/zuco/v2'\n",
    "PROC = BASE_DIR.parent / 'data/processed'\n",
    "RPTS = BASE_DIR / 'reports'\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "RPTS.mkdir(parents=True, exist_ok=True)\n",
    "WORDBOUNDS_CACHE = {}\n",
    "\n",
    "# --- Helper functions ---\n",
    "def heartbeat(m):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {m}\")\n",
    "\n",
    "def norm_token(s):\n",
    "    return re.sub(r\"[\\W_]+\", \"\", s.lower()) if isinstance(s, str) else s\n",
    "\n",
    "heartbeat('Libraries and paths are set up.')\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 2. Define Data Processing Functions\n",
    "Define all functions required for processing the data. This includes functions to read MATLAB .mat files (both standard and v7.3 HDF5), extract word-level text and timing data for both ZuCo v1 and v2, process individual subject sessions, and a main function to build the dataset in parallel using joblib.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "def _get_v2_word_texts(subject_id, task_name_long, version_path):\n",
    "    \"\"\"Extracts word texts from ZuCo v2 results.mat files (HDF5 format).\"\"\"\n",
    "    if not _HAS_H5PY:\n",
    "        return []\n",
    "    \n",
    "    task_name_short = task_name_long.split(' - ')[-1]\n",
    "    results_path = version_path / task_name_long / 'Matlab files' / f\"results{subject_id}_{task_name_short}.mat\"\n",
    "    \n",
    "    if not results_path.exists():\n",
    "        return []\n",
    "\n",
    "    words = []\n",
    "    try:\n",
    "        with h5py.File(results_path, 'r') as f:\n",
    "            if 'sentenceData' not in f or 'word' not in f['sentenceData']:\n",
    "                return []\n",
    "            \n",
    "            word_refs = f['sentenceData']['word']\n",
    "            for ref_array in word_refs:\n",
    "                for ref in ref_array:\n",
    "                    word_obj = f[ref]\n",
    "                    words.append(''.join(chr(c[0]) for c in word_obj))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading v2 word texts from {results_path}: {e}\")\n",
    "    return words\n",
    "\n",
    "def _find_wordbounds_file(version_path, subject_id, session_id):\n",
    "    \"\"\"Finds the wordbounds file with caching, checking for different naming conventions.\"\"\"\n",
    "    cache_key = str(version_path)\n",
    "    if cache_key not in WORDBOUNDS_CACHE:\n",
    "        WORDBOUNDS_CACHE[cache_key] = list(version_path.rglob('wordbounds*.mat'))\n",
    "    \n",
    "    # Generate possible filenames to check\n",
    "    task_short = re.sub(r'\\d+$', '', session_id)\n",
    "    alt_ids = {session_id, session_id.replace(\"SR\", \"SNR\"), session_id.replace(\"SNR\", \"SR\")}\n",
    "    \n",
    "    for s_id in alt_ids:\n",
    "        patterns = [\n",
    "            f\"wordbounds_{s_id}.mat\",\n",
    "            f\"wordbounds_{s_id}_{subject_id}.mat\",\n",
    "            f\"wordbounds_{task_short}_{subject_id}.mat\"\n",
    "        ]\n",
    "        for f_path in WORDBOUNDS_CACHE[cache_key]:\n",
    "            if f_path.name in patterns:\n",
    "                return f_path\n",
    "    return None\n",
    "\n",
    "def _process_session(eyedata, wb_data, tb_data, subject_id, task, version, word_texts=None):\n",
    "    \"\"\"Core logic to process fixations against word and sentence boundaries.\"\"\"\n",
    "    rows = []\n",
    "    word_idx_global = 0\n",
    "    min_latency = np.min(eyedata.fixations[:, 0])\n",
    "\n",
    "    for sent_idx, sent_data in enumerate(wb_data):\n",
    "        if version == 'v1':\n",
    "            words_in_sent = sent_data.content if wb_data.dtype == 'O' and hasattr(sent_data, 'content') else sent_data\n",
    "            n_words = len(words_in_sent)\n",
    "        else: # v2\n",
    "            n_words = len(sent_data)\n",
    "\n",
    "        if tb_data is None or sent_idx >= len(tb_data):\n",
    "            continue\n",
    "\n",
    "        sent_start, sent_end = tb_data[sent_idx][0], tb_data[sent_idx][1]\n",
    "        duration = (sent_end - sent_start) / n_words if n_words > 0 else 0\n",
    "\n",
    "        for word_idx_in_sent in range(n_words):\n",
    "            start_t = sent_start + (word_idx_in_sent * duration) + min_latency\n",
    "            end_t = start_t + duration\n",
    "            \n",
    "            if version == 'v1':\n",
    "                content = f'word_{word_idx_global}'\n",
    "            else: # v2\n",
    "                if word_texts and word_idx_global < len(word_texts):\n",
    "                    content = word_texts[word_idx_global]\n",
    "                else:\n",
    "                    continue # Skip if we've run out of text\n",
    "            \n",
    "            fixations = eyedata.fixations[(eyedata.fixations[:, 0] >= start_t) & (eyedata.fixations[:, 0] < end_t)]\n",
    "            \n",
    "            ffd = fixations[0, 2] if len(fixations) > 0 else 0\n",
    "            trt = np.sum(fixations[:, 2]) if len(fixations) > 0 else 0\n",
    "            gd = np.sum(fixations[:-1, 2]) if len(fixations) > 1 else ffd\n",
    "            \n",
    "            rows.append({'Subject': subject_id, 'Task': task, 'Dataset': version, 'Word': content, 'FFD': ffd, 'GD': gd, 'TRT': trt})\n",
    "            word_idx_global += 1\n",
    "            \n",
    "    return rows\n",
    "\n",
    "def _process_subject_session(subject_id, task_name, session_id, version_path_str):\n",
    "    \"\"\"Loads files for a single subject/session and orchestrates the processing.\"\"\"\n",
    "    version_path = Path(version_path_str)\n",
    "    if not _HAS_SCIPY: return \"scipy not installed\"\n",
    "\n",
    "    et_path = version_path / task_name / 'Preprocessed' / subject_id / f\"{subject_id}_{session_id}_corrected_ET.mat\"\n",
    "    if not et_path.exists(): return f\"ET file not found: {et_path.name}\"\n",
    "\n",
    "    wb_path = _find_wordbounds_file(version_path, subject_id, session_id)\n",
    "    if not wb_path: return f\"Wordbounds file not found for {subject_id}/{session_id}\"\n",
    "\n",
    "    try:\n",
    "        et_mat = loadmat(et_path, squeeze_me=True, struct_as_record=False)\n",
    "        wb_mat = loadmat(wb_path, squeeze_me=True, struct_as_record=False)\n",
    "    except Exception as e: return f\"Could not load .mat file: {e}\"\n",
    "\n",
    "    eyedata = et_mat.get('eyeevent')\n",
    "    wb_data = wb_mat.get('wordbounds')\n",
    "    tb_data = wb_mat.get('textbounds')\n",
    "\n",
    "    if not (eyedata and hasattr(eyedata, 'fixations') and wb_data is not None): return \"Invalid .mat structure\"\n",
    "    if hasattr(eyedata.fixations, 'data'): eyedata.fixations = eyedata.fixations.data\n",
    "\n",
    "    version = version_path.name\n",
    "    task_short = task_name.split(' - ')[-1].split('-')[-1].strip()\n",
    "    \n",
    "    word_texts = None\n",
    "    if version == 'v2':\n",
    "        word_texts = _get_v2_word_texts(subject_id, task_name, version_path)\n",
    "        if not word_texts: return \"V2 word texts not found.\"\n",
    "\n",
    "    rows = _process_session(eyedata, wb_data, tb_data, subject_id, task_short, version, word_texts)\n",
    "    \n",
    "    if not rows: return \"Failed to extract any valid word-level data.\"\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_dataset_parallel(version_path):\n",
    "    \"\"\"Discovers all sessions and processes them in parallel.\"\"\"\n",
    "    jobs = []\n",
    "    version_path_str = str(version_path.resolve())\n",
    "    \n",
    "    for et_path in version_path.rglob('*_corrected_ET.mat'):\n",
    "        match = re.match(r'(\\w+)_(\\w+)_corrected_ET\\.mat', et_path.name)\n",
    "        if match:\n",
    "            subj, sess = match.groups()\n",
    "            task = next((p.name for p in et_path.parents if 'task' in p.name.lower()), None)\n",
    "            if task:\n",
    "                jobs.append(delayed(_process_subject_session)(subj, task, sess, version_path_str))\n",
    "\n",
    "    if not jobs: return pd.DataFrame(), []\n",
    "    \n",
    "    heartbeat(f\"Processing {len(jobs)} sessions from {version_path.name}...\")\n",
    "    results = Parallel(n_jobs=-1)(jobs)\n",
    "    \n",
    "    dfs = [res for res in results if isinstance(res, pd.DataFrame)]\n",
    "    errors = [res for res in results if isinstance(res, str)]\n",
    "    \n",
    "    if not dfs: return pd.DataFrame(), errors\n",
    "        \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    heartbeat(f\"Completed {version_path.name}. Got {len(df)} rows from {len(dfs)} sessions.\")\n",
    "    return df, errors\n",
    "\n",
    "heartbeat('Data processing functions are defined.')\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 3. Process ZuCo v1 and v2 Datasets\n",
    "Execute the parallel processing function for both the ZuCo v1 and v2 raw data directories. This will process all subject sessions and create two separate pandas DataFrames, one for each dataset version, while reporting progress and any errors encountered.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# --- Main Execution: Build v1 & v2 in Parallel ---\n",
    "df_v1, errors_v1 = build_dataset_parallel(RAW_V1)\n",
    "df_v2, errors_v2 = build_dataset_parallel(RAW_V2)\n",
    "\n",
    "# --- Report Errors ---\n",
    "all_errors = {'v1': errors_v1, 'v2': errors_v2}\n",
    "for version, errors in all_errors.items():\n",
    "    if errors:\n",
    "        heartbeat(f\"Encountered {len(errors)} errors in {version}:\")\n",
    "        for err, count in pd.Series(errors).value_counts().items():\n",
    "            print(f\"  - [{count} sessions] {err}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 4. Unify, Clean, and Save Datasets\n",
    "Concatenate the v1 and v2 DataFrames into a single unified dataset. Apply a canonicalization function to normalize tokens and data types. Remove any duplicate rows. Save the individual v1, v2, and the final unified DataFrames to CSV files in the processed data directory.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "def canonicalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applies final cleaning and type conversions.\"\"\"\n",
    "    if df.empty: return df\n",
    "    df = df.copy()\n",
    "    df['token_norm'] = df['Word'].astype(str).map(norm_token)\n",
    "    for cat in ['Subject', 'Dataset', 'Task']:\n",
    "        df[cat] = df[cat].astype(str)\n",
    "    return df\n",
    "\n",
    "# --- Unify, Canonicalize, and Save ---\n",
    "all_df = pd.concat([d for d in [df_v1, df_v2] if not d.empty], ignore_index=True)\n",
    "\n",
    "if all_df.empty:\n",
    "    warnings.warn('No ZuCo data was processed. Check raw data paths and .mat files.')\n",
    "else:\n",
    "    all_df = canonicalize(all_df)\n",
    "    # Drop duplicates that might arise from processing errors\n",
    "    key_cols = ['Subject', 'Task', 'Word', 'FFD', 'GD', 'TRT']\n",
    "    all_df = all_df.drop_duplicates(subset=key_cols).reset_index(drop=True)\n",
    "    heartbeat(f\"Unified and canonicalized {len(all_df)} rows for {all_df['Subject'].nunique()} subjects.\")\n",
    "\n",
    "    # Save outputs\n",
    "    v1_out = PROC / 'zuco_v1_unified.csv'\n",
    "    v2_out = PROC / 'zuco_v2_unified.csv'\n",
    "    uni_out = PROC / 'zuco_unified_et_eeg.csv'\n",
    "    \n",
    "    if not df_v1.empty:\n",
    "        df_v1.to_csv(v1_out, index=False)\n",
    "        heartbeat(f\"Saved v1 data to {v1_out}\")\n",
    "    if not df_v2.empty:\n",
    "        df_v2.to_csv(v2_out, index=False)\n",
    "        heartbeat(f\"Saved v2 data to {v2_out}\")\n",
    "        \n",
    "    all_df.to_csv(uni_out, index=False)\n",
    "    heartbeat(f\"Saved unified data to {uni_out}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 5. Generate Quality Assurance Report\n",
    "Create a dictionary containing QA metrics such as total rows, number of subjects, data coverage for eye-tracking and EEG features, and file existence checks. Save this report as a JSON file for verification and record-keeping.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# --- QA Report ---\n",
    "qa_report = {}\n",
    "if 'all_df' in locals() and not all_df.empty:\n",
    "    v1_out = PROC / 'zuco_v1_unified.csv'\n",
    "    v2_out = PROC / 'zuco_v2_unified.csv'\n",
    "    uni_out = PROC / 'zuco_unified_et_eeg.csv'\n",
    "    \n",
    "    qa_report = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'files': {\n",
    "            'v1_output': str(v1_out), 'v1_exists': v1_out.exists(),\n",
    "            'v2_output': str(v2_out), 'v2_exists': v2_out.exists(),\n",
    "            'unified_output': str(uni_out), 'unified_exists': uni_out.exists(),\n",
    "        },\n",
    "        'row_counts': {'v1': len(df_v1), 'v2': len(df_v2), 'total': len(all_df)},\n",
    "        'subject_counts': {'v1': df_v1['Subject'].nunique() if not df_v1.empty else 0,\n",
    "                           'v2': df_v2['Subject'].nunique() if not df_v2.empty else 0,\n",
    "                           'total': all_df['Subject'].nunique()},\n",
    "        'et_coverage_pct': {c: f\"{all_df[c].notna().mean()*100:.1f}%\" for c in ['FFD','GD','TRT'] if c in all_df},\n",
    "        'error_summary': {\n",
    "            'v1_errors': pd.Series(errors_v1).value_counts().to_dict() if errors_v1 else {},\n",
    "            'v2_errors': pd.Series(errors_v2).value_counts().to_dict() if errors_v2 else {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    qa_path = RPTS / 'zuco_loader_qa_report.json'\n",
    "    with open(qa_path, 'w') as f:\n",
    "        json.dump(qa_report, f, indent=2)\n",
    "    \n",
    "    heartbeat(f\"Saved QA report to {qa_path}\")\n",
    "    print(\"--- QA Report ---\")\n",
    "    print(json.dumps(qa_report, indent=2))\n",
    "else:\n",
    "    warnings.warn(\"No data processed, skipping QA report.\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 6. Final Data Integrity Check\n",
    "Load the final unified CSV file that was just saved. Display the first few rows (`.head()`), the shape of the DataFrame, a summary of null values, and descriptive statistics for the key numerical columns to visually verify the output.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# --- Integrity Check ---\n",
    "final_output_path = PROC / 'zuco_unified_et_eeg.csv'\n",
    "\n",
    "if final_output_path.exists():\n",
    "    heartbeat(f\"Loading final unified data from {final_output_path} for integrity check...\")\n",
    "    check_df = pd.read_csv(final_output_path)\n",
    "\n",
    "    print(\"\\n--- Data Head ---\")\n",
    "    print(check_df.head())\n",
    "\n",
    "    print(f\"\\n--- Data Shape ---\")\n",
    "    print(f\"Rows: {check_df.shape[0]}, Columns: {check_df.shape[1]}\")\n",
    "\n",
    "    print(\"\\n--- Null Value Summary ---\")\n",
    "    print(check_df.isnull().sum())\n",
    "\n",
    "    print(\"\\n--- Descriptive Statistics for ET Metrics ---\")\n",
    "    et_cols = [c for c in ['FFD', 'GD', 'TRT'] if c in check_df.columns]\n",
    "    if et_cols:\n",
    "        print(check_df[et_cols].describe())\n",
    "    else:\n",
    "        print(\"No ET columns found to describe.\")\n",
    "    \n",
    "    heartbeat(\"Integrity check complete.\")\n",
    "else:\n",
    "    warnings.warn(f\"Final output file not found at {final_output_path}. Cannot perform integrity check.\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 7. (Optional) Data Recovery for Corrupted Files\n",
    "Provide a shell script cell using `wget` and `unzip` to re-download and extract the ZuCo v2 data from its official source. This step is for troubleshooting in case the local `.mat` files are corrupted, which can cause HDF5 loading errors.\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"bash\">\n",
    "# This cell is optional. Run it if you suspect the ZuCo v2 data files are corrupted.\n",
    "# It will download and overwrite the task 2 data.\n",
    "\n",
    "# Define paths\n",
    "ZIP_FILE=\"../data/raw_public/zuco/v2/task2-TSR.zip\"\n",
    "EXTRACT_DIR=\"../data/raw_public/zuco/v2/\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "mkdir -p $EXTRACT_DIR\n",
    "\n",
    "# Download the zip file from OSF\n",
    "echo \"Downloading ZuCo v2 Task 2 data from OSF...\"\n",
    "wget -O $ZIP_FILE \"https://osf.io/download/5e6b614f87b00100093a2199/\"\n",
    "\n",
    "# Unzip the file, overwriting existing files\n",
    "echo \"Extracting data... This will overwrite existing files.\"\n",
    "unzip -o $ZIP_FILE -d $EXTRACT_DIR\n",
    "\n",
    "echo \"Data recovery process complete. Please re-run the notebook from the top.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
