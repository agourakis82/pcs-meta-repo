{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "version": "3.x"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Notebook 03 \u2014 ZuCo Loader (v1 & v2 Unification, v4.3)\n", "\n", "**Purpose.** Load token-level ZuCo **v1** and **v2**, harmonize schema, and export a unified file for modeling.\n", "\n", "**Expected inputs (if available):**\n", "- `data/raw_public/zuco/v1/` and `data/raw_public/zuco/v2/` containing token-level CSV/TSV files; optionally `.mat` files (requires `scipy.io`).\n", "\n", "**Outputs:**\n", "- `data/processed/zuco_v1_aligned.csv`\n", "- `data/processed/zuco_v2_aligned.csv`\n", "- `data/processed/zuco_aligned.csv` (unified v1+v2)\n", "- `reports/zuco_loader_qa.json` (coverage & sanity checks)\n", "\n", "**Reproducibility.** Deterministic IO; schema normalization included.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["import os, time, warnings, re\n", "from pathlib import Path\n", "import numpy as np, pandas as pd\n", "RAW_V1 = Path('data/raw_public/zuco/v1')\n", "RAW_V2 = Path('data/raw_public/zuco/v2')\n", "PROC = Path('data/processed'); PROC.mkdir(parents=True, exist_ok=True)\n", "RPTS = Path('reports'); RPTS.mkdir(parents=True, exist_ok=True)\n", "def heartbeat(m): print(f\"[{time.strftime('%H:%M:%S')}] {m}\")\n", "def normcols(df): df.columns=[c.strip() for c in df.columns]; return df\n", "def norm_token(s):\n", "    if not isinstance(s,str): return s\n", "    s=s.lower(); s=re.sub(r'[\\W_]+','',s); return s\n", "COMMON=['Subject','SentenceID','Word','FFD','GD','TRT','GPT','ThetaPower','AlphaPower','Task','Dataset','length','log_freq','surprisal']\n", "heartbeat('Env ready')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["def _read_table_any(p: Path):\n", "    try:\n", "        if p.suffix.lower()=='.csv': return pd.read_csv(p)\n", "        if p.suffix.lower() in {'.tsv','.txt'}: return pd.read_csv(p, sep='\\t')\n", "    except Exception as e:\n", "        warnings.warn(f'Failed to read {p}: {e}')\n", "    return pd.DataFrame()\n", "def _ensure_cols(df, dataset_label: str, default_task='NR'):\n", "    if df.empty: return df\n", "    df=normcols(df)\n", "    ren={}\n", "    if 'word' in df and 'Word' not in df: ren['word']='Word'\n", "    if 'sentence_id' in df and 'SentenceID' not in df: ren['sentence_id']='SentenceID'\n", "    if 'subject' in df and 'Subject' not in df: ren['subject']='Subject'\n", "    df=df.rename(columns=ren)\n", "    if 'length' not in df and 'Word' in df: df['length']=df['Word'].astype(str).str.len()\n", "    for c in ['log_freq','surprisal','ThetaPower','AlphaPower','FFD','GD','TRT','GPT']:\n", "        if c not in df: df[c]=np.nan\n", "    if 'Task' not in df: df['Task']=default_task\n", "    df['Dataset']=dataset_label\n", "    keep=[c for c in COMMON if c in df.columns]\n", "    return df[keep]\n", "def build_zuco_v(folder: Path, dataset_label: str, default_task='NR'):\n", "    if not folder.exists(): return pd.DataFrame()\n", "    files=[]\n", "    for ext in ('*.csv','*.tsv','*.txt'): files+=list(folder.rglob(ext))\n", "    dfs=[]\n", "    for f in files:\n", "        d=_read_table_any(f)\n", "        if not d.empty: dfs.append(_ensure_cols(d, dataset_label, default_task))\n", "    if not dfs: return pd.DataFrame()\n", "    out=pd.concat(dfs, ignore_index=True)\n", "    if 'Subject' in out: out['Subject']=out['Subject'].astype(str)\n", "    if 'SentenceID' in out: out['SentenceID']=out['SentenceID'].astype(str)\n", "    return out\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["df_v1=build_zuco_v(RAW_V1,'v1','NR')\n", "df_v2=build_zuco_v(RAW_V2,'v2')\n", "if not df_v1.empty:\n", "    (PROC/'zuco_v1_aligned.csv').write_text(df_v1.to_csv(index=False)); heartbeat('Saved v1')\n", "if not df_v2.empty:\n", "    (PROC/'zuco_v2_aligned.csv').write_text(df_v2.to_csv(index=False)); heartbeat('Saved v2')\n", "df_all=pd.concat([d for d in [df_v1,df_v2] if not d.empty], ignore_index=True)\n", "if df_all.empty:\n", "    warnings.warn('No data unified.')\n", "else:\n", "    (PROC/'zuco_aligned.csv').write_text(df_all.to_csv(index=False)); heartbeat('Saved unified')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["qa={}\n", "qa['files']={'zuco_v1_aligned.csv': (PROC/'zuco_v1_aligned.csv').exists(),\n", "             'zuco_v2_aligned.csv': (PROC/'zuco_v2_aligned.csv').exists(),\n", "             'zuco_aligned.csv': (PROC/'zuco_aligned.csv').exists()}\n", "if (PROC/'zuco_aligned.csv').exists():\n", "    df=pd.read_csv(PROC/'zuco_aligned.csv')\n", "    for c in ['Subject','SentenceID','Word','Dataset','Task']:\n", "        if c not in df.columns: df[c]=np.nan\n", "    qa['rows_total']=int(len(df)); qa['subjects']=int(df['Subject'].nunique()) if 'Subject' in df else 0\n", "    qa['by_dataset']=df.groupby('Dataset').size().to_dict() if 'Dataset' in df else {}\n", "    qa['by_task']=df.groupby('Task').size().to_dict() if 'Task' in df else {}\n", "    # coverage vs KEC (if exists)\n", "    kec_path=PROC/'kec'/'metrics_en.csv'\n", "    if kec_path.exists():\n", "        k=pd.read_csv(kec_path)\n", "        for dft in (df,k): dft.columns=[c.strip() for c in dft.columns]\n", "        df['token_norm']=df['Word'].astype(str).str.lower().str.replace(r'[\\W_]+','',regex=True)\n", "        k['token_norm']=k['word'].astype(str).str.lower().str.replace(r'[\\W_]+','',regex=True) if 'word' in k.columns else np.nan\n", "        m=df.merge(k[['token_norm','entropy','curvature','coherence']], on='token_norm', how='left')\n", "        qa['kec_coverage_pct']={c: float(m[c].notna().mean()*100.0) for c in ['entropy','curvature','coherence']}\n", "    else:\n", "        qa['kec_coverage_pct']=None\n", "import json\n", "(Path('reports')/'zuco_loader_qa.json').write_text(json.dumps(qa, indent=2))\n", "heartbeat('Saved QA')\n"]}]}