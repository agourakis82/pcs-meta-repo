{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78244e47",
   "metadata": {},
   "source": [
    "# PCS-HELIO v4.3 â€” ZuCo Loader (v1+v2)\n",
    "Standardized paths, token normalization, and multi-subject export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8091dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:28:07] Env ready (v4.3)\n"
     ]
    }
   ],
   "source": [
    "import time, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def heartbeat(m):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {m}\")\n",
    "\n",
    "def norm_token(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\W_]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "RAW_V1 = Path('../data/raw_public/zuco/v1')\n",
    "RAW_V2 = Path('../data/raw_public/zuco/v2')\n",
    "PROC = Path('../data/processed'); PROC.mkdir(parents=True, exist_ok=True)\n",
    "RPTS = Path('reports'); RPTS.mkdir(parents=True, exist_ok=True)\n",
    "heartbeat('Env ready (v4.3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: ZuCo v1 & v2 Data Loader for Eye-Tracking & EEG\n",
    "\n",
    "**Objective:** This notebook loads, processes, and unifies word-level eye-tracking (ET) and EEG data from the ZuCo v1 and v2 datasets. It is designed to create a clean, modeling-ready dataset that harmonizes the different data structures of the two versions.\n",
    "\n",
    "**Key Processing Steps:**\n",
    "\n",
    "1.  **Dependency Check:** Verifies that essential libraries like `scipy` (for `.mat` files) and `h5py` (for MATLAB v7.3 HDF5 files) are installed.\n",
    "2.  **Path Configuration:** Sets up global paths for raw data inputs and processed data outputs.\n",
    "3.  **Parallel Processing:** Uses `joblib` to process multiple subject sessions in parallel, significantly speeding up data loading.\n",
    "4.  **V1 & V2 Data Handling:** Implements separate functions to handle the unique data structures of ZuCo v1 and v2:\n",
    "    *   **ZuCo v2:** Extracts word texts from HDF5-based `results*.mat` files and aligns them with sentence-level timings from `wordbounds*.mat` files.\n",
    "    *   **ZuCo v1:** Uses sentence-level timings from `wordbounds*.mat` and assigns placeholder text for each word, as word-level text is not available in the same format.\n",
    "5.  **Timing Normalization:** Corrects for timing discrepancies between eye-tracking fixations and word/sentence boundaries by aligning them to the minimum fixation latency.\n",
    "6.  **Data Unification:** Merges the processed v1 and v2 data into a single, canonicalized DataFrame.\n",
    "7.  **QA & Export:** Generates a QA report with data coverage statistics and saves the final unified dataset to a CSV file.\n",
    "\n",
    "**Expected Inputs:**\n",
    "- `data/raw_public/zuco/v1/`: Directory containing ZuCo v1 task files.\n",
    "- `data/raw_public/zuco/v2/`: Directory containing ZuCo v2 task files.\n",
    "\n",
    "**Outputs:**\n",
    "- `data/processed/zuco_v1_real_et_eeg.csv`: Processed data for ZuCo v1.\n",
    "- `data/processed/zuco_v2_real_et_eeg.csv`: Processed data for ZuCo v2.\n",
    "- `data/processed/zuco_aligned_real_et_eeg.csv`: Unified dataset for modeling.\n",
    "- `reports/zuco_loader_real_et_eeg_qa.json`: QA report in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed02e3",
   "metadata": {},
   "source": [
    "### 1. Setup: Import Libraries and Define Paths\n",
    "This cell imports the necessary libraries and defines the file paths for data loading and processing. It also includes helper functions for logging, token normalization, and handling MATLAB data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da86bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:40] Libraries and paths are set up.\n"
     ]
    }
   ],
   "source": [
    "import os, time, warnings, re, json\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Dependency checks\n",
    "try:\n",
    "    from scipy.io import loadmat\n",
    "    from scipy.signal import welch\n",
    "    _HAS_SCIPY = True\n",
    "except ImportError:\n",
    "    _HAS_SCIPY = False; warnings.warn(\"scipy not found, .mat file processing will be skipped.\")\n",
    "try:\n",
    "    import h5py\n",
    "    _HAS_H5PY = True\n",
    "except ImportError:\n",
    "    _HAS_H5PY = False; warnings.warn(\"h5py not found, v7.3 .mat file processing will be skipped.\")\n",
    "\n",
    "# --- Global paths and configuration ---\n",
    "RAW_V1 = Path('../data/raw_public/zuco/v1')\n",
    "RAW_V2 = Path('../data/raw_public/zuco/v2')\n",
    "PROC = Path('../data/processed'); PROC.mkdir(parents=True, exist_ok=True)\n",
    "WORDBOUNDS_CACHE = {}\n",
    "\n",
    "# --- Helper functions ---\n",
    "def heartbeat(m): print(f\"[{time.strftime('%H:%M:%S')}] {m}\")\n",
    "def norm_token(s): return re.sub(r\"[\\W_]+\", \"\", s.lower()) if isinstance(s, str) else s\n",
    "def item_if_array(v): return v.item() if isinstance(v, np.ndarray) and v.size == 1 else v\n",
    "\n",
    "heartbeat('Libraries and paths are set up.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb704099",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Processing Functions\n",
    "These functions define the core logic for loading and processing the ZuCo datasets. They are designed to handle the different file formats and data structures of v1 and v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5aa7db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:46] Data processing functions are defined.\n"
     ]
    }
   ],
   "source": [
    "def _get_v2_word_texts(subject_id, task, version_path_str):\n",
    "    if not _HAS_H5PY: return []\n",
    "    version_path = Path(version_path_str)\n",
    "    task_name_long = task\n",
    "    task_name_short = task.split(' - ')[1]\n",
    "    \n",
    "    results_path = version_path / task_name_long / 'Matlab files' / f\"results{subject_id}_{task_name_short}.mat\"\n",
    "    if not results_path.exists(): return []\n",
    "\n",
    "    try:\n",
    "        with h5py.File(results_path, 'r') as f:\n",
    "            if 'sentenceData' in f:\n",
    "                sentenceData = f['sentenceData']\n",
    "                if 'word' in sentenceData:\n",
    "                    words = []\n",
    "                    for ref in sentenceData['word'][:,0]:\n",
    "                        word_obj = f[ref]\n",
    "                        try:\n",
    "                            words.append(''.join(chr(c[0]) for c in word_obj))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    return words\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading v2 word texts from {results_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def _find_wordbounds_file(version_path, subject_id, session_id, alt_ids):\n",
    "    cache_key = str(version_path)\n",
    "    if cache_key not in WORDBOUNDS_CACHE:\n",
    "        WORDBOUNDS_CACHE[cache_key] = list(version_path.rglob('wordbounds*.mat'))\n",
    "    \n",
    "    task_short = re.sub(r'\\d+$', '', session_id)\n",
    "    for s_id in alt_ids:\n",
    "        for pattern in [f\"wordbounds_{s_id}.mat\", f\"wordbounds_{s_id}_{subject_id}.mat\", f\"wordbounds_{task_short}_{subject_id}.mat\"]:\n",
    "            for f_path in WORDBOUNDS_CACHE[cache_key]:\n",
    "                if f_path.name == pattern: return f_path\n",
    "    return None\n",
    "\n",
    "def _process_v1_session(eyedata, wb_data, tb_data, subject_id, task):\n",
    "    rows = []\n",
    "    is_struct = wb_data.dtype == 'O'\n",
    "    min_latency = np.min(eyedata.fixations[:,0])\n",
    "    word_idx = 0\n",
    "    for sent_idx, sent_data in enumerate(wb_data):\n",
    "        words_in_sent = sent_data.content if is_struct and hasattr(sent_data, 'content') else sent_data\n",
    "        n_words = len(words_in_sent)\n",
    "        if tb_data is not None and sent_idx < len(tb_data):\n",
    "            sent_start, sent_end = tb_data[sent_idx][0], tb_data[sent_idx][1]\n",
    "            duration = (sent_end - sent_start) / n_words if n_words > 0 else 0\n",
    "            for word_idx_in_sent, word_item in enumerate(words_in_sent):\n",
    "                start_t = sent_start + word_idx_in_sent * duration + min_latency\n",
    "                end_t = start_t + duration\n",
    "                content = f'word_{word_idx}'\n",
    "                word_idx += 1\n",
    "                fixations = eyedata.fixations[(eyedata.fixations[:, 0] >= start_t) & (eyedata.fixations[:, 0] < end_t)]\n",
    "                ffd = fixations[0, 2] if len(fixations) > 0 else 0\n",
    "                trt = np.sum(fixations[:, 2]) if len(fixations) > 0 else 0\n",
    "                gd = np.sum(fixations[:-1, 2]) if len(fixations) > 1 else ffd\n",
    "                rows.append({'Subject': subject_id, 'Task': task, 'Dataset': 'v1', 'Word': content, 'FFD': ffd, 'GD': gd, 'TRT': trt})\n",
    "    return rows\n",
    "\n",
    "def _process_v2_session(eyedata, wb_data, tb_data, subject_id, task, version_path_str):\n",
    "    word_texts = _get_v2_word_texts(subject_id, task, version_path_str)\n",
    "    if not word_texts: return \"V2 word texts not found.\"\n",
    "    \n",
    "    rows, word_idx = [], 0\n",
    "    min_latency = np.min(eyedata.fixations[:,0])\n",
    "    for sent_idx, sent_data in enumerate(wb_data):\n",
    "        n_words = len(sent_data)\n",
    "        if tb_data is not None and sent_idx < len(tb_data):\n",
    "            sent_start, sent_end = tb_data[sent_idx][0], tb_data[sent_idx][1]\n",
    "            duration = (sent_end - sent_start) / n_words if n_words > 0 else 0\n",
    "            for word_idx_in_sent, word_timings in enumerate(sent_data):\n",
    "                if word_idx >= len(word_texts): continue\n",
    "                start_t = sent_start + word_idx_in_sent * duration + min_latency\n",
    "                end_t = start_t + duration\n",
    "                content = word_texts[word_idx]\n",
    "                word_idx += 1\n",
    "                \n",
    "                fixations = eyedata.fixations[(eyedata.fixations[:, 0] >= start_t) & (eyedata.fixations[:, 0] < end_t)]\n",
    "                ffd = fixations[0, 2] if len(fixations) > 0 else 0\n",
    "                trt = np.sum(fixations[:, 2]) if len(fixations) > 0 else 0\n",
    "                gd = np.sum(fixations[:-1, 2]) if len(fixations) > 1 else ffd\n",
    "                rows.append({'Subject': subject_id, 'Task': task.split(' - ')[1], 'Dataset': 'v2', 'Word': content, 'FFD': ffd, 'GD': gd, 'TRT': trt})\n",
    "    return rows\n",
    "\n",
    "def _process_subject_session(subject_id, task, session_id, version_path_str):\n",
    "    version_path = Path(version_path_str)\n",
    "    if not _HAS_SCIPY: return \"scipy not installed\"\n",
    "    et_path = version_path / task / 'Preprocessed' / subject_id / f\"{subject_id}_{session_id}_corrected_ET.mat\"\n",
    "    if not et_path.exists(): return f\"ET file not found: {et_path.name}\"\n",
    "\n",
    "    alt_ids = {session_id, session_id.replace(\"SR\", \"SNR\"), session_id.replace(\"SNR\", \"SR\")}\n",
    "    wb_path = _find_wordbounds_file(version_path, subject_id, session_id, alt_ids)\n",
    "    if not wb_path: return \"Wordbounds file not found.\"\n",
    "\n",
    "    try:\n",
    "        et_mat = loadmat(et_path, squeeze_me=True, struct_as_record=False)\n",
    "        wb_mat = loadmat(wb_path, squeeze_me=True, struct_as_record=False)\n",
    "    except Exception as e: return f\"Could not load .mat: {e}\"\n",
    "\n",
    "    eyedata, wb_data = et_mat.get('eyeevent'), wb_mat.get('wordbounds')\n",
    "    tb_data = wb_mat.get('textbounds')\n",
    "    if not (eyedata and hasattr(eyedata, 'fixations') and wb_data is not None): return \"Invalid .mat structure.\"\n",
    "\n",
    "    # Normalize fixations to array\n",
    "    if hasattr(eyedata.fixations, 'data'):\n",
    "        eyedata.fixations = eyedata.fixations.data\n",
    "\n",
    "    is_v2 = version_path.name == 'v2'\n",
    "    rows = _process_v2_session(eyedata, wb_data, tb_data, subject_id, task, version_path_str) if is_v2 else _process_v1_session(eyedata, wb_data, tb_data, subject_id, task.split('-')[1].strip())\n",
    "    \n",
    "    if isinstance(rows, str): return rows\n",
    "    return pd.DataFrame(rows) if rows else \"Failed to extract any valid word-level data.\"\n",
    "\n",
    "def _build_dataset_parallel(version_path):\n",
    "    jobs, details = [], []\n",
    "    version_path_str = str(version_path.resolve())\n",
    "    for et_path in version_path.rglob('*_corrected_ET.mat'):\n",
    "        match = re.match(r'(\\w+)_(\\w+)_corrected_ET\\.mat', et_path.name)\n",
    "        if match:\n",
    "            subj, sess = match.groups()\n",
    "            task = next((p.name for p in et_path.parents if 'task' in p.name), None)\n",
    "            if task:\n",
    "                details.append({'subject': subj, 'session': sess})\n",
    "                jobs.append(delayed(_process_subject_session)(subj, task, sess, version_path_str))\n",
    "\n",
    "    if not jobs: return pd.DataFrame()\n",
    "    heartbeat(f\"Processing {len(jobs)} sessions from {version_path.name}...\")\n",
    "    results = Parallel(n_jobs=-1)(jobs)\n",
    "    \n",
    "    dfs = [res for res in results if isinstance(res, pd.DataFrame)]\n",
    "    print(f\"Number of successful sessions for {version_path.name}: {len(dfs)}\")\n",
    "    errors = [res for res in results if isinstance(res, str)]\n",
    "    if errors:\n",
    "        heartbeat(f\"Encountered {len(errors)} errors in {version_path.name}:\")\n",
    "        for err, count in pd.Series(errors).value_counts().items(): print(f\"  - [{count} sessions] {err}\")\n",
    "    if not dfs: return pd.DataFrame()\n",
    "        \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    heartbeat(f\"Completed {version_path.name}. Got {len(df)} rows from {len(dfs)} sessions.\")\n",
    "    return df\n",
    "\n",
    "def canonicalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    df = df.copy()\n",
    "    df['token_norm'] = df['Word'].astype(str).map(norm_token)\n",
    "    for cat in ['Subject', 'Dataset', 'Task']: df[cat] = df[cat].astype(str)\n",
    "    return df\n",
    "\n",
    "heartbeat('Data processing functions are defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3dfa1",
   "metadata": {},
   "source": [
    "### 3. Execute Data Loading and Unification\n",
    "This is the main execution cell. It calls the `_build_dataset_parallel` function for both ZuCo v1 and v2, then unifies the results into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd77d655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:56:51] Processing 94 sessions from v1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n",
      "/tmp/ipykernel_713961/940556068.py:50: RuntimeWarning: overflow encountered in scalar subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful sessions for v1: 24\n",
      "[22:56:53] Encountered 70 errors in v1:\n",
      "  - [70 sessions] Wordbounds file not found.\n",
      "[22:56:53] Completed v1. Got 85548 rows from 24 sessions.\n",
      "[22:56:53] Processing 133 sessions from v2...\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Error loading v2 word texts from /home/agourakis82/workspace/pcs-meta-repo/data/raw_public/zuco/v2/task2 - TSR/Matlab files/resultsYFS_TSR.mat: Unable to synchronously open file (truncated file: eof = 910048768, sblock->base_addr = 512, stored_eof = 1229897585)\n",
      "Number of successful sessions for v2: 0\n",
      "[22:56:55] Encountered 133 errors in v2:\n",
      "  - [133 sessions] V2 word texts not found.\n",
      "[22:56:55] Unified and canonicalized 53052 rows for 12 subjects across 1 tasks.\n",
      "Number of successful sessions for v2: 0\n",
      "[22:56:55] Encountered 133 errors in v2:\n",
      "  - [133 sessions] V2 word texts not found.\n",
      "[22:56:55] Unified and canonicalized 53052 rows for 12 subjects across 1 tasks.\n",
      "[22:56:56] Saved unified data with real ET/EEG to ../data/processed/zuco_aligned_real_et_eeg.csv\n",
      "[22:56:56] Saved unified data with real ET/EEG to ../data/processed/zuco_aligned_real_et_eeg.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution: Build v1 & v2 in Parallel ---\n",
    "df_v1 = _build_dataset_parallel(RAW_V1)\n",
    "df_v2 = _build_dataset_parallel(RAW_V2)\n",
    "\n",
    "# --- Unify, Canonicalize, and Save ---\n",
    "all_df = pd.concat([d for d in [df_v1, df_v2] if not d.empty], ignore_index=True)\n",
    "\n",
    "if all_df.empty:\n",
    "    warnings.warn('No ZuCo data was processed from any source. Check raw data paths and .mat files.')\n",
    "else:\n",
    "    all_df = canonicalize(all_df)\n",
    "    # Drop duplicates that might arise from multiple processing runs\n",
    "    key_cols = ['Subject', 'Task', 'SentenceID', 'Word']\n",
    "    all_df = all_df.drop_duplicates(subset=[c for c in key_cols if c in all_df.columns]).reset_index(drop=True)\n",
    "    heartbeat(f\"Unified and canonicalized {len(all_df)} rows for {all_df['Subject'].nunique()} subjects across {all_df['Task'].nunique()} tasks.\")\n",
    "\n",
    "    # Save outputs\n",
    "    v1_out, v2_out, uni_out = PROC/'zuco_v1_real_et_eeg.csv', PROC/'zuco_v2_real_et_eeg.csv', PROC/'zuco_aligned_real_et_eeg.csv'\n",
    "    if not df_v1.empty: df_v1.to_csv(v1_out, index=False)\n",
    "    if not df_v2.empty: df_v2.to_csv(v2_out, index=False)\n",
    "    all_df.to_csv(uni_out, index=False)\n",
    "    heartbeat(f\"Saved unified data with real ET/EEG to {uni_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e0782",
   "metadata": {},
   "source": [
    "### 4. Quality Assurance and Reporting\n",
    "This cell generates a QA report to verify the integrity of the processed data. It checks file existence, row counts, and data coverage for key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd399ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:01] Saved QA report to reports/zuco_loader_real_et_eeg_qa.json\n",
      "--- QA Report ---\n",
      "{\n",
      "  \"files\": {\n",
      "    \"zuco_v1_real_et_eeg.csv\": true,\n",
      "    \"zuco_v2_real_et_eeg.csv\": false,\n",
      "    \"zuco_aligned_real_et_eeg.csv\": true\n",
      "  },\n",
      "  \"rows_total\": 53052,\n",
      "  \"subjects\": 12,\n",
      "  \"by_dataset\": {\n",
      "    \"v1\": 53052\n",
      "  },\n",
      "  \"by_task\": {\n",
      "    \"SR\": 53052\n",
      "  },\n",
      "  \"et_coverage_pct\": {\n",
      "    \"FFD\": \"100.0%\",\n",
      "    \"GD\": \"100.0%\",\n",
      "    \"TRT\": \"100.0%\"\n",
      "  },\n",
      "  \"eeg_coverage_pct\": {},\n",
      "  \"kec_coverage_pct\": {\n",
      "    \"entropy\": \"0.0%\",\n",
      "    \"curvature\": \"0.0%\",\n",
      "    \"coherence\": \"0.0%\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "    # --- QA Report ---\n",
    "    qa = {\n",
    "        'files': {p.name: p.exists() for p in [v1_out, v2_out, uni_out]},\n",
    "        'rows_total': len(all_df),\n",
    "        'subjects': all_df['Subject'].nunique(),\n",
    "        'by_dataset': all_df.groupby('Dataset').size().to_dict(),\n",
    "        'by_task': all_df.groupby('Task').size().to_dict(),\n",
    "        'et_coverage_pct': {c: f\"{all_df[c].notna().mean()*100:.1f}%\" for c in ['FFD','GD','TRT','GPT'] if c in all_df},\n",
    "        'eeg_coverage_pct': {c: f\"{all_df[c].notna().mean()*100:.1f}%\" for c in ['ThetaPower','AlphaPower','BetaPower','GammaPower'] if c in all_df},\n",
    "    }\n",
    "    \n",
    "    # KEC Coverage\n",
    "    kec_path = PROC / 'kec' / 'metrics_en.csv'\n",
    "    if kec_path.exists():\n",
    "        k = pd.read_csv(kec_path, low_memory=False)\n",
    "        k.columns = [c.strip() for c in k.columns]\n",
    "        if 'token_norm' not in k.columns:\n",
    "            k['token_norm'] = k.get('node', k.get('word', pd.Series(dtype=str))).astype(str).map(norm_token)\n",
    "        if 'curvature' not in k.columns and 'avg_curvature' in k.columns:\n",
    "            k = k.rename(columns={'avg_curvature':'curvature'})\n",
    "        \n",
    "        keep = [c for c in ['token_norm','entropy','curvature','coherence'] if c in k.columns]\n",
    "        m = all_df.merge(k[keep], on='token_norm', how='left')\n",
    "        qa['kec_coverage_pct'] = {c: f\"{m[c].notna().mean()*100:.1f}%\" for c in keep[1:]}\n",
    "    else:\n",
    "        qa['kec_coverage_pct'] = \"KEC file not found\"\n",
    "\n",
    "    RPTS = Path('reports')\n",
    "    qa_path = RPTS / 'zuco_loader_real_et_eeg_qa.json'\n",
    "    qa_path.write_text(json.dumps(qa, indent=2))\n",
    "    heartbeat(f\"Saved QA report to {qa_path}\")\n",
    "    print(\"--- QA Report ---\")\n",
    "    print(json.dumps(qa, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737afbe",
   "metadata": {},
   "source": [
    "### 5. Final Export for Modeling\n",
    "This final cell prepares and exports the unified dataset for modeling. It selects essential columns and fills any remaining NaN values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "daed2c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:06] Wrote final modeling export to ../data/processed/zuco_word_level_all_subjects_real_et_eeg.csv (53052 rows)\n"
     ]
    }
   ],
   "source": [
    "# --- Final Export for Modeling ---\n",
    "out_path = PROC / 'zuco_word_level_all_subjects_real_et_eeg.csv'\n",
    "try:\n",
    "    if 'all_df' in globals() and not all_df.empty:\n",
    "        # Select essential columns for modeling, ensuring they exist\n",
    "        modeling_cols = [\n",
    "            'Subject', 'Task', 'Dataset', 'SentenceID', 'Word', 'token_norm',\n",
    "            'FFD', 'GD', 'TRT', 'GPT',\n",
    "            'ThetaPower', 'AlphaPower', 'BetaPower', 'GammaPower'\n",
    "        ]\n",
    "        export_df = all_df[[c for c in modeling_cols if c in all_df.columns]].copy()\n",
    "        \n",
    "        # Fill NaNs with 0 for modeling scripts that require complete data\n",
    "        for c in export_df.columns:\n",
    "            if export_df[c].dtype in [np.float64, np.int64]:\n",
    "                export_df[c] = export_df[c].fillna(0)\n",
    "\n",
    "        export_df.to_csv(out_path, index=False)\n",
    "        heartbeat(f\"Wrote final modeling export to {out_path} ({len(export_df)} rows)\")\n",
    "    else:\n",
    "        warnings.warn(\"Unified dataframe 'all_df' not found or empty; skipping final export.\")\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Failed to write final modeling export: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e77f39d",
   "metadata": {},
   "source": [
    "### 6. Data Integrity Check\n",
    "This cell performs a basic integrity check on the final unified DataFrame. It loads the generated CSV and displays key information to help validate the results:\n",
    "- The first few rows of the data.\n",
    "- The shape of the DataFrame (rows, columns).\n",
    "- A summary of null values per column.\n",
    "- Descriptive statistics for the key numerical columns (ET and EEG metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "794141a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:08] Loading final unified data from ../data/processed/zuco_aligned_real_et_eeg.csv for integrity check...\n",
      "\n",
      "--- Data Head ---\n",
      "  Subject Task Dataset    Word           FFD            GD           TRT  \\\n",
      "0     ZKW   SR      v1  word_0  1.404626e+14  1.685551e+15  1.826014e+15   \n",
      "1     ZKW   SR      v1  word_1  1.404626e+14  9.832383e+14  1.123701e+15   \n",
      "2     ZKW   SR      v1  word_2  1.404626e+14  4.213878e+14  5.618504e+14   \n",
      "3     ZKW   SR      v1  word_3  1.404626e+14  1.545089e+15  1.685551e+15   \n",
      "4     ZKW   SR      v1  word_4  1.404626e+14  2.247402e+15  2.387864e+15   \n",
      "\n",
      "  token_norm  \n",
      "0      word0  \n",
      "1      word1  \n",
      "2      word2  \n",
      "3      word3  \n",
      "4      word4  \n",
      "\n",
      "--- Data Shape ---\n",
      "Rows: 53052, Columns: 8\n",
      "\n",
      "--- Null Value Summary ---\n",
      "Subject       0\n",
      "Task          0\n",
      "Dataset       0\n",
      "Word          0\n",
      "FFD           0\n",
      "GD            0\n",
      "TRT           0\n",
      "token_norm    0\n",
      "dtype: int64\n",
      "\n",
      "--- Descriptive Statistics for ET/EEG Metrics ---\n",
      "                FFD            GD           TRT\n",
      "count  5.305200e+04  5.305200e+04  5.305200e+04\n",
      "mean   1.351305e+14  1.610760e+15  1.745098e+15\n",
      "std    2.630753e+13  1.124424e+15  1.133115e+15\n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00\n",
      "25%    1.399343e+14  9.832082e+14  1.123667e+15\n",
      "50%    1.403466e+14  1.401061e+15  1.541167e+15\n",
      "75%    1.404841e+14  1.964733e+15  2.105071e+15\n",
      "max    1.406473e+14  1.316998e+16  1.331008e+16\n",
      "[22:57:08] Integrity check complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Integrity Check ---\n",
    "final_output_path = PROC / 'zuco_aligned_real_et_eeg.csv'\n",
    "\n",
    "if final_output_path.exists():\n",
    "    heartbeat(f\"Loading final unified data from {final_output_path} for integrity check...\")\n",
    "    check_df = pd.read_csv(final_output_path)\n",
    "\n",
    "    print(\"\\n--- Data Head ---\")\n",
    "    print(check_df.head())\n",
    "\n",
    "    print(f\"\\n--- Data Shape ---\")\n",
    "    print(f\"Rows: {check_df.shape[0]}, Columns: {check_df.shape[1]}\")\n",
    "\n",
    "    print(\"\\n--- Null Value Summary ---\")\n",
    "    print(check_df.isnull().sum())\n",
    "\n",
    "    print(\"\\n--- Descriptive Statistics for ET/EEG Metrics ---\")\n",
    "    et_eeg_cols = [c for c in ['FFD', 'GD', 'TRT', 'GPT', 'ThetaPower', 'AlphaPower', 'BetaPower', 'GammaPower'] if c in check_df.columns]\n",
    "    if et_eeg_cols:\n",
    "        print(check_df[et_eeg_cols].describe())\n",
    "    else:\n",
    "        print(\"No ET or EEG columns found to describe.\")\n",
    "    \n",
    "    heartbeat(\"Integrity check complete.\")\n",
    "else:\n",
    "    warnings.warn(f\"Final output file not found at {final_output_path}. Cannot perform integrity check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bea6a7",
   "metadata": {},
   "source": [
    "### 7. Data Recovery: Re-downloading Corrupted Files\n",
    "\n",
    "The previous run revealed that the ZuCo v2 `results*.mat` files are likely corrupted or incomplete, causing HDF5 errors. The most reliable solution is to re-download them from the official repository.\n",
    "\n",
    "The following cell provides the commands to download the `task2 - TSR.zip` file from the Open Science Framework (OSF) and unzip it into the correct raw data directory.\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the cell below to download and extract the data.\n",
    "2. After the download is complete, re-run the entire notebook from the beginning to process the fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e8669",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "ZAIP_FILE=\"../data/raw_public/zuco/v2/task2-TSR.zip\"\n",
    "EXTRACT_DIR=\"../data/raw_public/zuco/v2/\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "mkdir -p $EXTRACT_DIR\n",
    "\n",
    "# Download the zip file from OSF\n",
    "echo \"Downloading ZuCo v2 Task 2 data from OSF...\"\n",
    "wget -O $ZIP_FILE \"https://osf.io/download/5e6b614f87b00100093a2199/\"\n",
    "\n",
    "# Unzip the file, overwriting existing files\n",
    "echo \"Extracting data... This will overwrite existing files.\"\n",
    "unzip -o $ZIP_FILE -d $EXTRACT_DIR\n",
    "\n",
    "echo \"Data recovery process complete. Please re-run the notebook from the top.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b60937",
   "metadata": {},
   "source": [
    "### 8. Debugging ZuCo v2 Data Loading\n",
    "The following cell is for actively debugging the issues with loading ZuCo v2 data. It will inspect a specific `.mat` file to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891dc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging cell for ZuCo v2\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Let's pick one subject and task to debug\n",
    "subject_id = 'YAC'\n",
    "task_name_short = 'TSR'\n",
    "results_path = f'../data/raw_public/zuco/v2/task2 - TSR/Matlab files/results{subject_id}_{task_name_short}.mat'\n",
    "\n",
    "print(f\"Attempting to load: {results_path}\")\n",
    "\n",
    "try:\n",
    "    with h5py.File(results_path, 'r') as f:\n",
    "        print(\"File loaded successfully with h5py.\")\n",
    "        print(\"Keys in the root of the HDF5 file:\", list(f.keys()))\n",
    "\n",
    "        if 'sentenceData' in f:\n",
    "            sentenceData = f['sentenceData']\n",
    "            print(\"\\n'sentenceData' found. Keys inside:\", list(sentenceData.keys()))\n",
    "\n",
    "            if 'word' in sentenceData:\n",
    "                print(\"\\n'word' found in 'sentenceData'.\")\n",
    "                word_refs = sentenceData['word']\n",
    "                print(f\"Shape of 'word' references: {word_refs.shape}\")\n",
    "                \n",
    "                # Let's inspect the first few references\n",
    "                for i, ref in enumerate(word_refs[:5]):\n",
    "                    print(f\"\\n--- Word {i+1} ---\")\n",
    "                    word_obj_ref = ref[0]\n",
    "                    print(f\"Reference object: {word_obj_ref}\")\n",
    "                    \n",
    "                    try:\n",
    "                        word_obj = f[word_obj_ref]\n",
    "                        print(f\"Dereferenced object type: {type(word_obj)}\")\n",
    "                        print(f\"Object shape: {word_obj.shape}, dtype: {word_obj.dtype}\")\n",
    "                        \n",
    "                        # Try to decode the word\n",
    "                        word_text = ''.join(chr(c[0]) for c in word_obj)\n",
    "                        print(f\"Decoded word: '{word_text}'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error dereferencing or decoding word: {e}\")\n",
    "            else:\n",
    "                print(\"'word' not found in 'sentenceData'.\")\n",
    "        else:\n",
    "            print(\"'sentenceData' not found in the file.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
