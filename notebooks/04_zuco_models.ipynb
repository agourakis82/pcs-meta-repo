{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "pygments_lexer": "ipython3", "version": "3.x" }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "# Notebook 04 — ZuCo Models: Reading Cost & EEG vs KEC (v4.3, Upgraded)\n",
     "\n",
     "**Purpose.** Validate KEC metrics (transition entropy, local curvature, meso-coherence) contra ZuCo (ET+EEG) com:\n",
     "- OLS + **SE robustos clusterizados** (Subject)\n",
     "- **FDR** (Benjamini–Hochberg)\n",
     "- **Bootstrap paralelizado** (joblib)\n",
     "- **MixedLM** (random intercepts Subject + variance components SentenceID)\n",
     "\n",
     "**Inputs esperados:**\n",
     "- `data/processed/kec/metrics_en.csv`\n",
     "- `data/processed/zuco_aligned.csv`\n",
     "\n",
     "**Saídas:**\n",
     "- `figures/metrics/F2_reading_vs_KEC.png`\n",
     "- `figures/metrics/F3_EEG_vs_KEC.png`\n",
     "- `data/processed/models_reading_coeffs.csv`\n",
     "- `data/processed/models_reading_coeffs_fdr.csv`\n",
     "- `data/processed/boot_ols_ffd_entropy.csv` (se bootstrap executado)\n",
     "- `data/processed/mixedlm_ffd_summary.txt` (se MixedLM executado)\n",
     "- `reports/qa_kec_models.json`\n"
   ]
  },

  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "import os, time, warnings\n",
     "from pathlib import Path\n",
     "import numpy as np\n",
     "import pandas as pd\n",
     "import statsmodels.api as sm\n",
     "import statsmodels.formula.api as smf\n",
     "import matplotlib.pyplot as plt\n",
     "\n",
     "SEED = 42\n",
     "np.random.seed(SEED)\n",
     "\n",
     "FIG_DIR = Path('figures/metrics'); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
     "PROC_DIR = Path('data/processed'); PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
     "REPORTS_DIR = Path('reports'); REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
     "\n",
     "def heartbeat(msg):\n",
     "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
     "heartbeat('Environment ready')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1) Load & merge (KEC + ZuCo)"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "kec_path = Path('data/processed/kec/metrics_en.csv')\n",
     "zuco_path = Path('data/processed/zuco_aligned.csv')\n",
     "if not kec_path.exists(): warnings.warn(f'Missing {kec_path}')\n",
     "if not zuco_path.exists(): warnings.warn(f'Missing {zuco_path}')\n",
     "kec_df = pd.read_csv(kec_path) if kec_path.exists() else pd.DataFrame()\n",
     "zuco_df = pd.read_csv(zuco_path) if zuco_path.exists() else pd.DataFrame()\n",
     "\n",
     "def normcols(df):\n",
     "    df.columns=[c.strip() for c in df.columns]; return df\n",
     "kec_df = normcols(kec_df); zuco_df = normcols(zuco_df)\n",
     "\n",
     "kec_word_col = 'word' if 'word' in kec_df.columns else ('Word' if 'Word' in kec_df.columns else None)\n",
     "zuco_word_col = 'Word' if 'Word' in zuco_df.columns else ('word' if 'word' in zuco_df.columns else None)\n",
     "\n",
     "if len(kec_df)>0 and len(zuco_df)>0 and kec_word_col and zuco_word_col:\n",
     "    merged = zuco_df.merge(kec_df, left_on=zuco_word_col, right_on=kec_word_col, how='left', suffixes=('', '_kec'))\n",
     "else:\n",
     "    merged = pd.DataFrame()\n",
     "\n",
     "heartbeat(f'Merged: {merged.shape if len(merged)>0 else \"N/A\"}')\n",
     "merged.head(3) if len(merged)>0 else merged"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2) Prepare columns & transforms"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "if len(merged)==0:\n",
     "    heartbeat('Merged empty — fill data and rerun.')\n",
     "\n",
     "cols = merged.columns.tolist()\n",
     "ET_COLS = [c for c in ['FFD','GD','TRT','GPT'] if c in cols]\n",
     "EEG_COLS = [c for c in ['ThetaPower','AlphaPower'] if c in cols]\n",
     "KEC_COLS = [c for c in ['entropy','curvature','coherence'] if c in cols]\n",
     "if 'LogFreq' in merged: merged = merged.rename(columns={'LogFreq':'log_freq'})\n",
     "\n",
     "needed = set(ET_COLS + KEC_COLS + ['Subject','SentenceID'])\n",
     "df = merged.copy()\n",
     "if len(df)>0 and needed.issubset(df.columns):\n",
     "    df = df.dropna(subset=list(needed))\n",
     "else:\n",
     "    df = merged\n",
     "\n",
     "if 'Subject' in df: df['Subject']=df['Subject'].astype(str)\n",
     "if 'SentenceID' in df: df['SentenceID']=df['SentenceID'].astype(str)\n",
     "\n",
     "for resp in ['TRT','GPT']:\n",
     "    if resp in df: df[f'log_{resp}'] = np.log1p(df[resp])\n",
     "\n",
     "heartbeat(f'ET: {ET_COLS}; EEG: {EEG_COLS}; KEC: {KEC_COLS}')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3) OLS (clustered SE por Subject) + export coeficientes"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "reading_results = []\n",
     "if len(df)>0 and 'Subject' in df:\n",
     "    resp_list = [r for r in ['FFD','GD','log_TRT','log_GPT'] if r in df.columns]\n",
     "    for resp in resp_list:\n",
     "        preds = [c for c in ['entropy','curvature','coherence','length','log_freq','surprisal'] if c in df.columns]\n",
     "        if len(preds)<3:\n",
     "            heartbeat(f'Predictors missing for {resp}; skipping.')\n",
     "            continue\n",
     "        formula = f\"{resp} ~ \" + \" + \".join(preds)\n",
     "        heartbeat(f'Fitting OLS: {formula}')\n",
     "        try:\n",
     "            model = smf.ols(formula, data=df).fit()\n",
     "            rob = model.get_robustcov_results(cov_type='cluster', groups=df['Subject'])\n",
     "            coefs = rob.params.rename('coef').to_frame(); coefs['se']=rob.bse; coefs['t']=rob.tvalues; coefs['p']=rob.pvalues; coefs['response']=resp\n",
     "            reading_results.append(coefs.reset_index().rename(columns={'index':'term'}))\n",
     "        except Exception as e:\n",
     "            warnings.warn(f'OLS failed for {resp}: {e}')\n",
     "\n",
     "if reading_results:\n",
     "    reading_tbl = pd.concat(reading_results, ignore_index=True)\n",
     "    out_csv = PROC_DIR/'models_reading_coeffs.csv'\n",
     "    reading_tbl.to_csv(out_csv, index=False)\n",
     "    heartbeat(f'Saved → {out_csv}')\n",
     "else:\n",
     "    heartbeat('No reading models fitted.')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4) F2 — Partial effect (FFD ~ entropy) com covariáveis medianas"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "def partial_plot(df, response='FFD', xvar='entropy', controls=None, n_points=100, savepath=None):\n",
     "    if controls is None: controls=[]\n",
     "    need=[response,xvar]+controls\n",
     "    if not set(need).issubset(df.columns):\n",
     "        heartbeat(f'Missing columns for partial plot: {need}')\n",
     "        return\n",
     "    formula = response + ' ~ ' + xvar\n",
     "    if controls: formula += ' + ' + ' + '.join(controls)\n",
     "    mod = smf.ols(formula, data=df.dropna(subset=need)).fit()\n",
     "    xgrid = np.linspace(df[xvar].quantile(0.05), df[xvar].quantile(0.95), n_points)\n",
     "    base = {c: float(df[c].median()) for c in controls}\n",
     "    grid_df = pd.DataFrame({xvar: xgrid, **base}); yhat = mod.predict(grid_df)\n",
     "    plt.figure(); samp = df.dropna(subset=need).sample(min(2000,len(df)), random_state=SEED)\n",
     "    plt.scatter(samp[xvar], samp[response], alpha=0.2); plt.plot(xgrid, yhat)\n",
     "    plt.xlabel(xvar); plt.ylabel(response)\n",
     "    ttl = ', '.join([f\"{c}@median\" for c in controls]) if controls else 'no controls'\n",
     "    plt.title(f'Partial: {response} ~ {xvar} ({ttl})')\n",
     "    if savepath: plt.tight_layout(); plt.savefig(savepath, dpi=150); heartbeat(f'Saved → {savepath}')\n",
     "    plt.show()\n",
     "\n",
     "if len(df)>0 and {'FFD','entropy'}.issubset(df.columns):\n",
     "    controls = [c for c in ['length','log_freq'] if c in df.columns]\n",
     "    partial_plot(df, response='FFD', xvar='entropy', controls=controls, savepath=FIG_DIR/'F2_reading_vs_KEC.png')\n",
     "else:\n",
     "    heartbeat('Skipping F2 — required columns missing.')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5) F3 — EEG vs entropy (por sujeito)"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "def per_subject_trends(df, eeg_col='ThetaPower', xvar='entropy', savepath=None):\n",
     "    need={'Subject',eeg_col,xvar}\n",
     "    if not need.issubset(df.columns):\n",
     "        heartbeat(f'Missing for F3: {need}'); return\n",
     "    subs = sorted(df['Subject'].unique()); nsub=len(subs)\n",
     "    if nsub==0: heartbeat('No subjects for F3'); return\n",
     "    import math\n",
     "    ncols=4; nrows=int(math.ceil(nsub/ncols))\n",
     "    plt.figure(figsize=(12, 3*nrows))\n",
     "    for i,s in enumerate(subs,1):\n",
     "        ax = plt.subplot(nrows, ncols, i)\n",
     "        sdf = df[df['Subject']==s].dropna(subset=[eeg_col,xvar])\n",
     "        if len(sdf)<5: ax.set_title(f'{s} (few data)'); continue\n",
     "        ax.scatter(sdf[xvar], sdf[eeg_col], alpha=0.3)\n",
     "        try:\n",
     "            m = smf.ols(f\"{eeg_col} ~ {xvar}\", data=sdf).fit()\n",
     "            xs = np.linspace(sdf[xvar].min(), sdf[xvar].max(), 50)\n",
     "            ys = m.params['Intercept'] + m.params[xvar]*xs\n",
     "            ax.plot(xs, ys)\n",
     "        except Exception:\n",
     "            pass\n",
     "        ax.set_title(str(s)); ax.set_xlabel(xvar); ax.set_ylabel(eeg_col)\n",
     "    plt.tight_layout()\n",
     "    if savepath: plt.savefig(savepath, dpi=150); heartbeat(f'Saved → {savepath}')\n",
     "    plt.show()\n",
     "\n",
     "if len(df)>0 and 'entropy' in df.columns and any(c in df.columns for c in ['ThetaPower','AlphaPower']):\n",
     "    eegc = 'ThetaPower' if 'ThetaPower' in df.columns else 'AlphaPower'\n",
     "    per_subject_trends(df, eeg_col=eegc, xvar='entropy', savepath=FIG_DIR/'F3_EEG_vs_KEC.png')\n",
     "else:\n",
     "    heartbeat('Skipping F3 — EEG/entropy missing.')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6) QA JSON"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "qa = {\n",
     "  'n_rows_merged': int(len(merged)) if len(merged)>0 else 0,\n",
     "  'et_cols': ET_COLS if 'ET_COLS' in globals() else [],\n",
     "  'eeg_cols': EEG_COLS if 'EEG_COLS' in globals() else [],\n",
     "  'kec_cols': KEC_COLS if 'KEC_COLS' in globals() else [],\n",
     "  'reading_coeffs_csv': 'data/processed/models_reading_coeffs.csv' if (PROC_DIR/'models_reading_coeffs.csv').exists() else None,\n",
     "  'fig_F2': 'figures/metrics/F2_reading_vs_KEC.png' if (FIG_DIR/'F2_reading_vs_KEC.png').exists() else None,\n",
     "  'fig_F3': 'figures/metrics/F3_EEG_vs_KEC.png' if (FIG_DIR/'F3_EEG_vs_KEC.png').exists() else None\n",
     "}\n",
     "with open(REPORTS_DIR/'qa_kec_models.json', 'w') as f:\n",
     "    import json; json.dump(qa, f, indent=2)\n",
     "heartbeat('Saved → reports/qa_kec_models.json')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "## 7) (Opcional) MixedLM (random intercepts Subject + vc SentenceID)\n",
     "Executa um modelo confirmatório para FFD com KEC + controles, contabilizando repetição por sujeito e sentenças."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "from statsmodels.regression.mixed_linear_model import MixedLM\n",
     "def fit_mixedlm_ffd(df):\n",
     "    cols_req = {'FFD','entropy','curvature','coherence','length','log_freq','Subject','SentenceID'}\n",
     "    if not cols_req.issubset(df.columns):\n",
     "        print('MixedLM skipped — columns missing.'); return None\n",
     "    mdf = df[list(cols_req)].dropna().copy()\n",
     "    exog = sm.add_constant(mdf[['entropy','curvature','coherence','length','log_freq']])\n",
     "    endog = mdf['FFD']\n",
     "    vc = {'Sentence': '0 + C(SentenceID)'}\n",
     "    model = MixedLM(endog, exog, groups=mdf['Subject'], vc_formula=vc)\n",
     "    try:\n",
     "        result = model.fit(reml=True, method='lbfgs')\n",
     "        print(result.summary())\n",
     "        out_txt = Path('data/processed')/'mixedlm_ffd_summary.txt'\n",
     "        with open(out_txt, 'w') as f: f.write(str(result.summary()))\n",
     "        print(f'Saved MixedLM summary → {out_txt}')\n",
     "        return result\n",
     "    except Exception as e:\n",
     "        print(f'MixedLM failed: {e}'); return None\n",
     "\n",
     "# Uso: result = fit_mixedlm_ffd(df)"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "## 8) (Opcional) Bootstrap paralelizado (joblib)\n",
     "Bootstrap por cluster (Subject) para CIs de FFD ~ entropy + length + log_freq."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "from joblib import Parallel, delayed\n",
     "def _boot_fit_once(groups, df, formula, rng):\n",
     "    samp_groups = rng.choice(groups, size=len(groups), replace=True)\n",
     "    samp_df = pd.concat([df[df['Subject']==g] for g in samp_groups], ignore_index=True)\n",
     "    try:\n",
     "        m = smf.ols(formula, data=samp_df).fit()\n",
     "        return m.params\n",
     "    except Exception:\n",
     "        return None\n",
     "\n",
     "def bootstrap_coefs_parallel(df, formula, group_col='Subject', B=1000, n_jobs=-1, seed=42):\n",
     "    rng = np.random.default_rng(seed)\n",
     "    groups = df[group_col].unique()\n",
     "    seeds = rng.integers(low=0, high=2**32-1, size=B, endpoint=True)\n",
     "    results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
     "        delayed(_boot_fit_once)(groups, df, formula, np.random.default_rng(int(s))) for s in seeds\n",
     "    )\n",
     "    params = [r for r in results if r is not None]\n",
     "    return pd.DataFrame(params)\n",
     "\n",
     "# Exemplo (execute manualmente se quiser rodar):\n",
     "# if len(df)>0 and {'FFD','entropy','length','log_freq','Subject'}.issubset(df.columns):\n",
     "#     boot = bootstrap_coefs_parallel(df[['FFD','entropy','length','log_freq','Subject']].dropna(),\n",
     "#                                     'FFD ~ entropy + length + log_freq',\n",
     "#                                     B=1000, n_jobs=-1)\n",
     "#     if len(boot)>0:\n",
     "#         boot.to_csv('data/processed/boot_ols_ffd_entropy.csv', index=False)\n",
     "#         heartbeat('Saved bootstrap → data/processed/boot_ols_ffd_entropy.csv')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "## 9) FDR correction (Benjamini–Hochberg)\n",
     "Ajusta p-values por família (FFD, GD, log_TRT, log_GPT)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": [
     "from statsmodels.stats.multitest import multipletests\n",
     "reading_csv = PROC_DIR/'models_reading_coeffs.csv'\n",
     "if reading_csv.exists():\n",
     "    dfc = pd.read_csv(reading_csv)\n",
     "    outs = []\n",
     "    for resp, grp in dfc.groupby('response'):\n",
     "        pvals = grp['p'].values\n",
     "        rej, p_adj, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
     "        g2 = grp.copy(); g2['p_fdr_bh']=p_adj; g2['rej_fdr_bh_0.05']=rej\n",
     "        outs.append(g2)\n",
     "    df_fdr = pd.concat(outs, ignore_index=True)\n",
     "    out_csv = PROC_DIR/'models_reading_coeffs_fdr.csv'\n",
     "    df_fdr.to_csv(out_csv, index=False)\n",
     "    heartbeat(f'Saved FDR → {out_csv}')\n",
     "else:\n",
     "    heartbeat('FDR skipped — models_reading_coeffs.csv not found.')"
   ]
  }
 ]
}
