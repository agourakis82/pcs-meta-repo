{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04 — ZuCo Models: Reading Cost & EEG vs KEC (v4.3)\n",
    "\n",
    "**Purpose.** Validate KEC metrics (transition entropy, local curvature, meso-coherence) against ZuCo eye-tracking (FFD, GD, TRT, GPT) and EEG.\n",
    "\n",
    "**Inputs (expected):**\n",
    "- `data/processed/kec/metrics_en.csv` — per-word KEC metrics (English)\n",
    "- `data/processed/zuco_aligned.csv` — per-token ZuCo features (ET+EEG)\n",
    "\n",
    "**Outputs:**\n",
    "- Figures: `figures/metrics/F2_reading_vs_KEC.png`, `figures/metrics/F3_EEG_vs_KEC.png`\n",
    "- Tables: `data/processed/models_reading_coeffs.csv`, `data/processed/models_eeg_coeffs.csv` (if EEG present)\n",
    "- QA JSON: `reports/qa_kec_models.json` (optional)\n",
    "\n",
    "**Reproducibility:** fixed seeds; minimal dependencies; deterministic IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] Environment ready\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, warnings, hashlib, time, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "FIG_DIR = Path('../figures/metrics'); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR = Path('../data/processed'); PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR = Path('../reports'); REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def heartbeat(msg):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "heartbeat('Environment ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load processed data and merge\n",
    "Assumes the KEC table and the ZuCo-aligned table exist under `/data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:01] Loaded KEC: (166540, 4); ZuCo combined: (112074, 21)\n",
      "[21:21:01] Merged shape: (161064, 24); sources used: ['../data/processed/zuco_word_level_all_subjects.csv', '../data/processed/zuco_aligned.csv', '../data/processed/zuco_v1_aligned.csv']\n",
      "[21:21:01] Merged shape: (161064, 24); sources used: ['../data/processed/zuco_word_level_all_subjects.csv', '../data/processed/zuco_aligned.csv', '../data/processed/zuco_v1_aligned.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Task</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>SentenceID</th>\n",
       "      <th>Word</th>\n",
       "      <th>FFD</th>\n",
       "      <th>GD</th>\n",
       "      <th>ThetaPower</th>\n",
       "      <th>AlphaPower</th>\n",
       "      <th>token_norm</th>\n",
       "      <th>...</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>WordPosition</th>\n",
       "      <th>SentenceID.1</th>\n",
       "      <th>TRT</th>\n",
       "      <th>GPT</th>\n",
       "      <th>control</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>entropy</th>\n",
       "      <th>coherence</th>\n",
       "      <th>curvature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZAB</td>\n",
       "      <td>NR</td>\n",
       "      <td>v1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>He</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>he</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.916100</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>553.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZAB</td>\n",
       "      <td>NR</td>\n",
       "      <td>v1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>won</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>won</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.857516</td>\n",
       "      <td>0.452555</td>\n",
       "      <td>705.692982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZAB</td>\n",
       "      <td>NR</td>\n",
       "      <td>v1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>the</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.219484</td>\n",
       "      <td>0.479532</td>\n",
       "      <td>964.234483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject Task Dataset SentenceID Word  FFD   GD  ThetaPower  AlphaPower  \\\n",
       "0     ZAB   NR      v1    unknown   He  0.0  0.0         0.0         0.0   \n",
       "1     ZAB   NR      v1    unknown  won  0.0  0.0         0.0         0.0   \n",
       "2     ZAB   NR      v1    unknown  the  0.0  0.0         0.0         0.0   \n",
       "\n",
       "  token_norm  ... relation_type WordPosition  SentenceID.1 TRT GPT  control  \\\n",
       "0         he  ...           NaN          NaN           NaN NaN NaN      NaN   \n",
       "1        won  ...           NaN          NaN           NaN NaN NaN      NaN   \n",
       "2        the  ...           NaN          NaN           NaN NaN NaN      NaN   \n",
       "\n",
       "  sentence_id   entropy  coherence   curvature  \n",
       "0         NaN  4.916100   0.378378  553.340426  \n",
       "1         NaN  5.857516   0.452555  705.692982  \n",
       "2         NaN  6.219484   0.479532  964.234483  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kec_path = PROC_DIR/'kec'/'metrics_en.csv'\n",
    "# Prefer multi-subject export; include v1 and v2 aligned if present\n",
    "zuco_candidates = [\n",
    "    PROC_DIR/'zuco_word_level_all_subjects.csv',\n",
    "    PROC_DIR/'zuco_aligned.csv',\n",
    "    PROC_DIR/'zuco_v1_aligned.csv',\n",
    "    PROC_DIR/'zuco_v2_aligned.csv',\n",
    "]\n",
    "\n",
    "# Load KEC\n",
    "kec_df = pd.read_csv(kec_path, low_memory=False) if kec_path.exists() else pd.DataFrame()\n",
    "\n",
    "# Load and combine ZuCo (v1+v2 if available)\n",
    "zuco_parts = []\n",
    "for p in zuco_candidates:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            dfi = pd.read_csv(p, low_memory=False)\n",
    "            dfi.columns = [c.strip() for c in dfi.columns]\n",
    "            # ensure subject col (lowercase) for this notebook\n",
    "            if 'subject' not in dfi.columns and 'Subject' in dfi.columns:\n",
    "                dfi['subject'] = dfi['Subject'].astype(str)\n",
    "            # ensure tsr indicator if possible\n",
    "            if 'tsr' not in dfi.columns:\n",
    "                if 'Task' in dfi.columns:\n",
    "                    dfi['tsr'] = dfi['Task'].astype(str)\n",
    "                else:\n",
    "                    dfi['tsr'] = 'Unknown'\n",
    "            # ensure Dataset exists\n",
    "            if 'Dataset' not in dfi.columns:\n",
    "                # infer from filename\n",
    "                name = p.name\n",
    "                if 'v2' in name:\n",
    "                    dfi['Dataset'] = 'v2'\n",
    "                elif 'v1' in name:\n",
    "                    dfi['Dataset'] = 'v1'\n",
    "                else:\n",
    "                    dfi['Dataset'] = 'v1'\n",
    "            zuco_parts.append(dfi)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f'Failed reading {p}: {e}')\n",
    "\n",
    "zuco_df = pd.concat(zuco_parts, ignore_index=True) if zuco_parts else pd.DataFrame()\n",
    "heartbeat(f\"Loaded KEC: {kec_df.shape if len(kec_df)>0 else 'MISSING'}; ZuCo combined: {zuco_df.shape if len(zuco_df)>0 else 'MISSING'}\")\n",
    "\n",
    "import re\n",
    "\n",
    "def norm_token(s):\n",
    "    if not isinstance(s,str): return s\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[\\W_]+', '', s)\n",
    "    return s\n",
    "\n",
    "# normalize headers\n",
    "for d in (kec_df, zuco_df):\n",
    "    if len(d)>0: d.columns = [c.strip() for c in d.columns]\n",
    "\n",
    "# token_norm sources for ZuCo\n",
    "if len(zuco_df)>0:\n",
    "    zuco_word_col = 'word' if 'word' in zuco_df.columns else ('Word' if 'Word' in zuco_df.columns else None)\n",
    "    if zuco_word_col:\n",
    "        zuco_df['token_norm'] = zuco_df[zuco_word_col].astype(str).map(norm_token)\n",
    "\n",
    "# token_norm for KEC (prefer existing, else fallbacks)\n",
    "if len(kec_df)>0:\n",
    "    kec_df.columns = [c.strip() for c in kec_df.columns]\n",
    "    kec_word_col = 'token_norm' if 'token_norm' in kec_df.columns else (\n",
    "        'node' if 'node' in kec_df.columns else (\n",
    "        'word' if 'word' in kec_df.columns else (\n",
    "        'Word' if 'Word' in kec_df.columns else None)))\n",
    "    if kec_word_col and 'token_norm' not in kec_df.columns:\n",
    "        kec_df['token_norm'] = kec_df[kec_word_col].astype(str).map(norm_token)\n",
    "\n",
    "# curvature alias\n",
    "work_kec = kec_df.rename(columns={'avg_curvature':'curvature'}) if len(kec_df)>0 else kec_df\n",
    "\n",
    "req = {'token_norm','entropy','coherence','curvature'}\n",
    "if len(zuco_df)>0 and len(work_kec)>0 and req.issubset(work_kec.columns) and 'token_norm' in zuco_df.columns:\n",
    "    merged = zuco_df.merge(work_kec[['token_norm','entropy','coherence','curvature']], on='token_norm', how='left', suffixes=('','_kec'))\n",
    "else:\n",
    "    merged = pd.DataFrame()\n",
    "heartbeat(f\"Merged shape: {merged.shape if len(merged)>0 else 'N/A'}; sources used: {[str(p) for p in zuco_candidates if p.exists()]}\")\n",
    "merged.head(3) if len(merged)>0 else merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prepare variables\n",
    "- Define response variables and EEG columns (if available).\n",
    "- Define predictors: KEC (`entropy`, `curvature`, `coherence`) and covariates (`length`, `log_freq`, optional `surprisal`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] ET columns: []; EEG columns: ['ThetaPower', 'AlphaPower']; KEC: ['entropy', 'coherence']\n"
     ]
    }
   ],
   "source": [
    "if len(merged)==0:\n",
    "    heartbeat('Merged DF empty — fill data and rerun.')\n",
    "\n",
    "col_candidates = merged.columns.tolist()\n",
    "ET_COLS = [c for c in ['duration_ms','fixation_samples','avg_gaze_x','avg_gaze_y','avg_pupil_area'] if c in col_candidates]  # Available ET metrics\n",
    "EEG_COLS = [c for c in ['ThetaPower','AlphaPower'] if c in col_candidates]  # EEG if available\n",
    "KEC_COLS = [c for c in ['entropy','avg_curvature','coherence'] if c in col_candidates]  # KEC metrics (note: curvature is avg_curvature)\n",
    "COV_COLS = [c for c in ['length','LogFreq','log_freq','surprisal','word_idx','subject','tsr'] if c in col_candidates]\n",
    "if 'LogFreq' in merged: merged = merged.rename(columns={'LogFreq':'log_freq'})\n",
    "\n",
    "needed = set(ET_COLS + KEC_COLS + ['subject','tsr'])  # Updated column names\n",
    "df = merged.copy()\n",
    "if len(df)>0 and needed.issubset(set(df.columns)):\n",
    "    df = df.dropna(subset=list(needed))\n",
    "    heartbeat(f\"Rows after NA drop: {len(df)}\")\n",
    "else:\n",
    "    df = merged\n",
    "\n",
    "if 'subject' in df: df['subject'] = df['subject'].astype(str)\n",
    "if 'tsr' in df: df['tsr'] = df['tsr'].astype(str)\n",
    "\n",
    "for resp in ['duration_ms','fixation_samples']:\n",
    "    if resp in df:\n",
    "        df[f'log_{resp}'] = np.log1p(df[resp])\n",
    "\n",
    "heartbeat(f\"ET columns: {ET_COLS}; EEG columns: {EEG_COLS}; KEC: {KEC_COLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Baseline OLS with clustered robust SE (Subject)\n",
    "Fit OLS for ET metrics (FFD/GD/log_TRT/log_GPT) with KEC + covariates and compute clustered robust SEs by Subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] No reading models were fitted.\n"
     ]
    }
   ],
   "source": [
    "reading_results = []\n",
    "if len(df)>0 and set(['subject']).issubset(df.columns):\n",
    "    resp_list = [r for r in ['duration_ms','fixation_samples','log_duration_ms','log_fixation_samples'] if r in df.columns]\n",
    "    for resp in resp_list:\n",
    "        preds = [c for c in ['entropy','avg_curvature','coherence','word_idx'] if c in df.columns]\n",
    "        if 'avg_curvature' in preds and 'curvature' not in df.columns:\n",
    "            df = df.rename(columns={'avg_curvature':'curvature'})\n",
    "            preds = [c if c!='avg_curvature' else 'curvature' for c in preds]\n",
    "        preds = [p for p in preds if p in df.columns]\n",
    "        if len(preds) < 1:\n",
    "            heartbeat(f\"Insufficient predictors for {resp}; skipping.\")\n",
    "            continue\n",
    "        formula = f\"{resp} ~ \" + \" + \".join(preds)\n",
    "        heartbeat(f\"Fitting OLS: {formula}\")\n",
    "        try:\n",
    "            used = [resp] + preds + (['subject'] if 'subject' in df.columns else [])\n",
    "            d = df.dropna(subset=[c for c in used if c in df.columns])\n",
    "            if len(d) < 100:\n",
    "                heartbeat(f\"Skip {resp}: insufficient rows after dropna ({len(d)})\")\n",
    "                continue\n",
    "            model = smf.ols(formula, data=d).fit()\n",
    "            coefs = pd.DataFrame({\n",
    "                'coef': model.params,\n",
    "                'se': model.bse,\n",
    "                't': model.tvalues,\n",
    "                'p': model.pvalues\n",
    "            })\n",
    "            coefs['response'] = resp\n",
    "            reading_results.append(coefs.reset_index().rename(columns={'index':'term'}))\n",
    "            heartbeat(f\"Model done for {resp}\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"OLS failed for {resp}: {e}\")\n",
    "\n",
    "if reading_results:\n",
    "    reading_tbl = pd.concat(reading_results, ignore_index=True)\n",
    "    out_csv = Path('../data/processed') / 'models_reading_coeffs.csv'\n",
    "    reading_tbl.to_csv(out_csv, index=False)\n",
    "    heartbeat(f\"Saved reading coefficients → {out_csv}\")\n",
    "else:\n",
    "    heartbeat('No reading models were fitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) (Optional) Mixed-Effects Models\n",
    "Use MixedLM for random intercepts by Subject (and optionally SentenceID) if needed. (Placeholder — disabled by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "if len(df)>0 and 'duration_ms' in df.columns:\n",
    "     try:\n",
    "         mdf = df[['duration_ms','entropy','avg_curvature','coherence','word_idx','subject']].dropna()\n",
    "         exog = sm.add_constant(mdf[['entropy','avg_curvature','coherence','word_idx']])\n",
    "         endog = mdf['duration_ms']\n",
    "         groups = mdf['subject']\n",
    "         mixed = MixedLM(endog, exog, groups=groups).fit()\n",
    "         print(mixed.summary())\n",
    "     except Exception as e:\n",
    "         warnings.warn(f'MixedLM failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Figure F2 — Reading Cost vs KEC (partial effect)\n",
    "Plot predicted relationship holding covariates constant (median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] Skipping F2 (duration_ms~entropy) — required columns missing.\n"
     ]
    }
   ],
   "source": [
    "def partial_plot(df, response='FFD', xvar='entropy', controls=None, n_points=100, savepath=None):\n",
    "    if controls is None: controls = []\n",
    "    cols_needed = [response, xvar] + controls\n",
    "    if not set(cols_needed).issubset(df.columns):\n",
    "        heartbeat(f\"Missing columns for partial plot: {cols_needed}\")\n",
    "        return\n",
    "    formula = response + ' ~ ' + xvar\n",
    "    if controls:\n",
    "        formula += ' + ' + ' + '.join(controls)\n",
    "    mod = smf.ols(formula, data=df.dropna(subset=cols_needed)).fit()\n",
    "    xgrid = np.linspace(df[xvar].quantile(0.05), df[xvar].quantile(0.95), n_points)\n",
    "    base = {c: float(df[c].median()) for c in controls}\n",
    "    grid_df = pd.DataFrame({xvar: xgrid, **base})\n",
    "    yhat = mod.predict(grid_df)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    samp = df.dropna(subset=cols_needed).sample(min(2000, len(df)), random_state=SEED)\n",
    "    plt.scatter(samp[xvar], samp[response], alpha=0.2)\n",
    "    plt.plot(xgrid, yhat)\n",
    "    plt.xlabel(xvar); plt.ylabel(response)\n",
    "    ttl_ctrl = ', '.join([f\"{c} @ median\" for c in controls]) if controls else 'no controls'\n",
    "    plt.title(f\"Partial effect: {response} ~ {xvar} ({ttl_ctrl})\")\n",
    "    if savepath:\n",
    "        plt.tight_layout(); plt.savefig(savepath, dpi=150)\n",
    "        heartbeat(f\"Saved figure → {savepath}\")\n",
    "    plt.show()\n",
    "\n",
    "if len(df)>0 and 'duration_ms' in df.columns and 'entropy' in df.columns:\n",
    "    controls = [c for c in ['word_idx'] if c in df.columns]\n",
    "    partial_plot(df, response='duration_ms', xvar='entropy', controls=controls, savepath=Path('../figures/metrics')/'F2_reading_vs_KEC.png')\n",
    "else:\n",
    "    heartbeat('Skipping F2 (duration_ms~entropy) — required columns missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Figure F3 — EEG vs KEC (per-subject trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] Skipping F3 — pupil area or entropy missing.\n"
     ]
    }
   ],
   "source": [
    "def per_subject_trends(df, eeg_col='avg_pupil_area', xvar='entropy', savepath=None):\n",
    "    need = {'subject', eeg_col, xvar}\n",
    "    if not need.issubset(df.columns):\n",
    "        heartbeat(f\"Missing columns for F3: {need}\")\n",
    "        return\n",
    "    subs = sorted(df['subject'].unique())\n",
    "    nsub = len(subs)\n",
    "    if nsub==0:\n",
    "        heartbeat('No subjects found for F3.')\n",
    "        return\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "    ncols = 4\n",
    "    nrows = int(math.ceil(nsub / ncols))\n",
    "    plt.figure(figsize=(12, 3*nrows))\n",
    "    for i, s in enumerate(subs, 1):\n",
    "        ax = plt.subplot(nrows, ncols, i)\n",
    "        sdf = df[df['subject']==s].dropna(subset=[eeg_col, xvar])\n",
    "        if len(sdf)<5:\n",
    "            ax.set_title(f\"{s} (insufficient data)\")\n",
    "            continue\n",
    "        ax.scatter(sdf[xvar], sdf[eeg_col], alpha=0.3)\n",
    "        try:\n",
    "            m = smf.ols(f\"{eeg_col} ~ {xvar}\", data=sdf).fit()\n",
    "            xs = np.linspace(sdf[xvar].min(), sdf[xvar].max(), 50)\n",
    "            ys = m.params['Intercept'] + m.params[xvar]*xs\n",
    "            ax.plot(xs, ys)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        ax.set_title(str(s)); ax.set_xlabel(xvar); ax.set_ylabel(eeg_col)\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=150)\n",
    "        heartbeat(f\"Saved figure → {savepath}\")\n",
    "    plt.show()\n",
    "\n",
    "if len(df)>0 and {'subject','entropy'}.issubset(df.columns) and any(c in df.columns for c in ['avg_pupil_area']):\n",
    "    eegc = 'avg_pupil_area'  # Use pupil area as proxy for EEG if no EEG data\n",
    "    per_subject_trends(df, eeg_col=eegc, xvar='entropy', savepath=Path('../figures/metrics')/'F3_EEG_vs_KEC.png')\n",
    "else:\n",
    "    heartbeat('Skipping F3 — pupil area or entropy missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) (Optional) Bootstrap CIs (clustered by Subject)\n",
    "Bootstrap coefficients for `FFD ~ entropy + length + log_freq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] Skipping bootstrap — required columns missing.\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_coefs(df, formula, group_col='subject', B=200, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    groups = df[group_col].unique()\n",
    "    coefs = []\n",
    "    for b in range(B):\n",
    "        samp_groups = rng.choice(groups, size=len(groups), replace=True)\n",
    "        samp_df = pd.concat([df[df[group_col]==g] for g in samp_groups], ignore_index=True)\n",
    "        try:\n",
    "            m = smf.ols(formula, data=samp_df).fit()\n",
    "            coefs.append(m.params)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame(coefs)\n",
    "\n",
    "if len(df)>0 and {'duration_ms','entropy','word_idx','subject'}.issubset(df.columns):\n",
    "    formula = 'duration_ms ~ entropy + word_idx'\n",
    "    heartbeat('Bootstrapping duration_ms model (B=200) ...')\n",
    "    boot = bootstrap_coefs(df[['duration_ms','entropy','word_idx','subject']].dropna(), formula, group_col='subject', B=200)\n",
    "    if len(boot)>0:\n",
    "        outb = Path('../data/processed')/'boot_ols_duration_entropy.csv'\n",
    "        boot.to_csv(outb, index=False)\n",
    "        heartbeat(f\"Saved bootstrap coefs → {outb}\")\n",
    "else:\n",
    "    heartbeat('Skipping bootstrap — required columns missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) QA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:15] Saved QA JSON → ../reports/qa_kec_models.json\n"
     ]
    }
   ],
   "source": [
    "qa = {\n",
    "  'timestamp_utc': '2025-09-01T18:23:49.464537Z',\n",
    "  'n_rows_merged': int(len(merged)) if len(merged)>0 else 0,\n",
    "  'et_cols': ET_COLS if 'ET_COLS' in globals() else [],\n",
    "  'eeg_cols': EEG_COLS if 'EEG_COLS' in globals() else [],\n",
    "  'kec_cols': KEC_COLS if 'KEC_COLS' in globals() else [],\n",
    "  'reading_coeffs_csv': '../data/processed/models_reading_coeffs.csv' if Path('../data/processed/models_reading_coeffs.csv').exists() else None,\n",
    "  'reading_coeffs_fdr_csv': '../data/processed/models_reading_coeffs_fdr.csv' if Path('../data/processed/models_reading_coeffs_fdr.csv').exists() else None,\n",
    "  'boot_ffd_entropy': '../data/processed/boot_ols_duration_entropy.csv' if Path('../data/processed/boot_ols_duration_entropy.csv').exists() else None,\n",
    "  'fig_F2': '../figures/metrics/F2_reading_vs_KEC.png' if Path('../figures/metrics/F2_reading_vs_KEC.png').exists() else None,\n",
    "  'fig_F3': '../figures/metrics/F3_EEG_vs_KEC.png' if Path('../figures/metrics/F3_EEG_vs_KEC.png').exists() else None\n",
    "}\n",
    "with open('../reports/qa_kec_models.json', 'w') as f:\n",
    "    json.dump(qa, f, indent=2)\n",
    "heartbeat(\"Saved QA JSON → ../reports/qa_kec_models.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Extend to GD, TRT (`log_TRT`), GPT (`log_GPT`) and other EEG bands as needed.\n",
    "- Prefer MixedLM with random intercepts for `Subject` and `SentenceID` for confirmatory models.\n",
    "- Keep seeds fixed; increase bootstrap `B` for narrower CIs (runtime ↑).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcs-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
