# Create Notebook 04 (ZuCo Models) according to v4.3 spec, with reproducible structure and outputs.
import os, json, textwrap, pathlib, datetime

base_notebooks = "/mnt/data/notebooks"
base_figs = "/mnt/data/figures/metrics"
base_data_proc = "/mnt/data/data/processed"
os.makedirs(base_notebooks, exist_ok=True)
os.makedirs(base_figs, exist_ok=True)
os.makedirs(base_data_proc, exist_ok=True)

now = datetime.datetime.utcnow().isoformat() + "Z"

nb = {
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.x"
    }
  },
  "cells": [
# Title and context
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 04 — ZuCo Models: Reading Cost & EEG vs KEC (v4.3)\n",
        "\n",
        "**Purpose.** Validate KEC metrics (transition entropy, local curvature, meso-coherence) against ZuCo eye-tracking (FFD, GD, TRT, GPT) and EEG.\n",
        "\n",
        "**Inputs (expected):**\n",
        "- `data/processed/kec/metrics_en.csv` — per-word KEC metrics (English)\n",
        "- `data/processed/zuco_aligned.csv` — per-token ZuCo features (ET+EEG)\n",
        "\n",
        "**Outputs:**\n",
        "- Figures: `figures/metrics/F2_reading_vs_KEC.png`, `figures/metrics/F3_EEG_vs_KEC.png`\n",
        "- Tables: `data/processed/models_reading_coeffs.csv`, `data/processed/models_eeg_coeffs.csv` (if EEG present)\n",
        "- QA JSON: `reports/qa_kec_models.json` (optional)\n",
        "\n",
        "**Reproducibility:** fixed seeds; minimal dependencies; deterministic IO."
      ]
    },
# Setup
  {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "import os, json, math, warnings, hashlib, time, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "FIG_DIR = Path('figures/metrics'); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROC_DIR = Path('data/processed'); PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REPORTS_DIR = Path('reports'); REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def heartbeat(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "heartbeat('Environment ready')"
      ]
    },
# Load data
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load processed data and merge\n",
        "Assumes the KEC table and the ZuCo-aligned table exist under `/data/processed/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "kec_path = Path('data/processed/kec/metrics_en.csv')\n",
        "zuco_path = Path('data/processed/zuco_aligned.csv')\n",
        "\n",
        "if not kec_path.exists():\n",
        "    warnings.warn(f\"KEC file not found: {kec_path}. Please generate it in Notebook 02.\")\n",
        "if not zuco_path.exists():\n",
        "    warnings.warn(f\"ZuCo aligned file not found: {zuco_path}. Please generate it in Notebook 03.\")\n",
        "\n",
        "kec_df = pd.read_csv(kec_path) if kec_path.exists() else pd.DataFrame()\n",
        "zuco_df = pd.read_csv(zuco_path) if zuco_path.exists() else pd.DataFrame()\n",
        "heartbeat(f\"Loaded KEC: {kec_df.shape if len(kec_df)>0 else 'MISSING'}; ZuCo: {zuco_df.shape if len(zuco_df)>0 else 'MISSING'}\")\n",
        "\n",
        "# Normalize column names\n",
        "def normcols(df):\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df\n",
        "kec_df = normcols(kec_df)\n",
        "zuco_df = normcols(zuco_df)\n",
        "\n",
        "# Heuristic column mapping\n",
        "kec_word_col = 'word' if 'word' in kec_df.columns else ('Word' if 'Word' in kec_df.columns else None)\n",
        "zuco_word_col = 'Word' if 'Word' in zuco_df.columns else ('word' if 'word' in zuco_df.columns else None)\n",
        "\n",
        "merge_cols = { 'word_left': zuco_word_col, 'word_right': kec_word_col }\n",
        "if len(kec_df)>0 and len(zuco_df)>0 and kec_word_col and zuco_word_col:\n",
        "    merged = zuco_df.merge(kec_df, left_on=zuco_word_col, right_on=kec_word_col, how='left', suffixes=('', '_kec'))\n",
        "else:\n",
        "    merged = pd.DataFrame()\n",
        "\n",
        "heartbeat(f\"Merged shape: {merged.shape if len(merged)>0 else 'N/A'}\")\n",
        "merged.head(3) if len(merged)>0 else merged"
      ]
    },
# Prepare variables
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Prepare variables\n",
        "- Define response variables (ET: FFD, GD, TRT, GPT) and EEG (e.g., ThetaPower).\n",
        "- Define predictors: KEC components (`entropy`, `curvature`, `coherence`) and covariates (`length`, `log_freq`, optional `surprisal`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "if len(merged)==0:\n",
        "    heartbeat('Merged DF empty — skipping modeling cells. Fill data and rerun.')\n",
        "\n",
        "# Try to locate columns\n",
        "col_candidates = merged.columns.tolist()\n",
        "\n",
        "ET_COLS = [c for c in ['FFD','GD','TRT','GPT'] if c in col_candidates]\n",
        "EEG_COLS = [c for c in ['ThetaPower','AlphaPower'] if c in col_candidates]\n",
        "KEC_COLS = [c for c in ['entropy','curvature','coherence'] if c in col_candidates]\n",
        "COV_COLS = [c for c in ['length','LogFreq','log_freq','surprisal','Position','Subject','SentenceID'] if c in col_candidates]\n",
        "\n",
        "# Standardize covariate names\n",
        "if 'LogFreq' in merged: merged = merged.rename(columns={'LogFreq':'log_freq'})\n",
        "\n",
        "# Drop rows missing essentials\n",
        "needed = set(ET_COLS + KEC_COLS + ['Subject','SentenceID'])\n",
        "df = merged.copy()\n",
        "if len(df)>0 and needed.issubset(set(df.columns)):\n",
        "    df = df.dropna(subset=list(needed))\n",
        "    heartbeat(f\"Rows after NA drop: {len(df)}\")\n",
        "else:\n",
        "    df = merged\n",
        "\n",
        "# Ensure types\n",
        "if 'Subject' in df: df['Subject'] = df['Subject'].astype(str)\n",
        "if 'SentenceID' in df: df['SentenceID'] = df['SentenceID'].astype(str)\n",
        "\n",
        "# Optionally log-transform ET metrics with heavy skew\n",
        "for resp in ['TRT','GPT']:\n",
        "    if resp in df:\n",
        "        df[f'log_{resp}'] = np.log1p(df[resp])\n",
        "\n",
        "heartbeat(f\"ET columns: {ET_COLS}; EEG columns: {EEG_COLS}; KEC: {KEC_COLS}\")"
      ]
    },
# OLS + cluster robust example
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Baseline OLS with clustered robust SE (Subject)\n",
        "We fit an OLS for an eye-tracking metric (e.g., FFD) with KEC + covariates, then compute **clustered robust SEs** by subject."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "reading_results = []\n",
        "if len(df)>0 and set(['Subject']).issubset(df.columns):\n",
        "    # Choose one response to demonstrate; extend to GD/TRT/GPT in a loop\n",
        "    resp_list = [r for r in ['FFD','GD','log_TRT','log_GPT'] if r in df.columns]\n",
        "    for resp in resp_list:\n",
        "        # Build formula dynamically\n",
        "        preds = [c for c in ['entropy','curvature','coherence','length','log_freq','surprisal'] if c in df.columns]\n",
        "        if len(preds)<3:\n",
        "            heartbeat(f\"Insufficient predictors present for {resp}; skipping.\")\n",
        "            continue\n",
        "        formula = f\"{resp} ~ \" + \" + \".join(preds)\n",
        "        heartbeat(f\"Fitting OLS: {formula}\")\n",
        "        try:\n",
        "            model = smf.ols(formula, data=df).fit()\n",
        "            # Clustered robust SE by Subject\n",
        "            rob = model.get_robustcov_results(cov_type='cluster', groups=df['Subject'])\n",
        "            # Collect coefficients\n",
        "            coefs = rob.params.rename('coef').to_frame()\n",
        "            coefs['se'] = rob.bse\n",
        "            coefs['t'] = rob.tvalues\n",
        "            coefs['p'] = rob.pvalues\n",
        "            coefs['response'] = resp\n",
        "            reading_results.append(coefs.reset_index().rename(columns={'index':'term'}))\n",
        "            heartbeat(f\"Model done for {resp}\")\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"OLS failed for {resp}: {e}\")\n",
        "\n",
        "if reading_results:\n",
        "    reading_tbl = pd.concat(reading_results, ignore_index=True)\n",
        "    out_csv = PROC_DIR / 'models_reading_coeffs.csv'\n",
        "    reading_tbl.to_csv(out_csv, index=False)\n",
        "    heartbeat(f\"Saved reading coefficients → {out_csv}\")\n",
        "else:\n",
        "    heartbeat('No reading models were fitted.')"
      ]
    },
# Mixed-effects demonstration (optional)
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) (Optional) Mixed-Effects Models\n",
        "For larger analyses, use linear mixed-effects (random intercepts for Subject / Sentence). Here we include a minimal example; full LME fitting can be CPU-intensive.\n",
        "\n",
        "> Tip: For production runs, prefer `lme4` in R or `pymer4` wrappers or `statsmodels` MixedLM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "# Minimal placeholder — uncomment and adapt if MixedLM is desired.\n",
        "# from statsmodels.regression.mixed_linear_model import MixedLM\n",
        "# if len(df)>0 and 'FFD' in df.columns:\n",
        "#     try:\n",
        "#         mdf = df[['FFD','entropy','curvature','coherence','length','log_freq','Subject']].dropna()\n",
        "#         exog = sm.add_constant(mdf[['entropy','curvature','coherence','length','log_freq']])\n",
        "#         endog = mdf['FFD']\n",
        "#         groups = mdf['Subject']\n",
        "#         mixed = MixedLM(endog, exog, groups=groups).fit()\n",
        "#         heartbeat('MixedLM fitted for FFD (random intercept by Subject).')\n",
        "#     except Exception as e:\n",
        "#         warnings.warn(f'MixedLM failed: {e}')"
      ]
    },
# Partial effect plot (F2)
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Figure F2 — Reading Cost vs KEC (partial effect)\n",
        "Plot predicted relationship of a reading metric vs a KEC component holding covariates constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "def partial_plot(df, response='FFD', xvar='entropy', controls=None, n_points=100, savepath=None):\n",
        "    if controls is None: controls = []\n",
        "    cols_needed = [response, xvar] + controls\n",
        "    if not set(cols_needed).issubset(df.columns):\n",
        "        heartbeat(f\"Missing columns for partial plot: {cols_needed}\")\n",
        "        return\n",
        "    # Fit OLS for partial relationship\n",
        "    formula = response + ' ~ ' + xvar\n",
        "    if controls:\n",
        "        formula += ' + ' + ' + '.join(controls)\n",
        "    mod = smf.ols(formula, data=df.dropna(subset=cols_needed)).fit()\n",
        "    # Build grid\n",
        "    xgrid = np.linspace(df[xvar].quantile(0.05), df[xvar].quantile(0.95), n_points)\n",
        "    base = {c: float(df[c].median()) for c in controls}\n",
        "    grid_df = pd.DataFrame({xvar: xgrid, **base})\n",
        "    yhat = mod.predict(grid_df)\n",
        "    # Plot\n",
        "    plt.figure()\n",
        "    # scatter (downsample for speed)\n",
        "    samp = df.dropna(subset=cols_needed).sample(min(2000, len(df)), random_state=SEED)\n",
        "    plt.scatter(samp[xvar], samp[response], alpha=0.2)\n",
        "    plt.plot(xgrid, yhat)\n",
        "    plt.xlabel(xvar)\n",
        "    plt.ylabel(response)\n",
        "    ttl_ctrl = ', '.join([f\"{c} @ median\" for c in controls]) if controls else 'no controls'\n",
        "    plt.title(f\"Partial effect: {response} ~ {xvar} ({ttl_ctrl})\")\n",
        "    if savepath:\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(savepath, dpi=150)\n",
        "        heartbeat(f\"Saved figure → {savepath}\")\n",
        "    plt.show()\n",
        "\n",
        "if len(df)>0 and 'FFD' in df.columns and 'entropy' in df.columns:\n",
        "    controls = [c for c in ['length','log_freq'] if c in df.columns]\n",
        "    partial_plot(df, response='FFD', xvar='entropy', controls=controls, savepath=FIG_DIR/'F2_reading_vs_KEC.png')\n",
        "else:\n",
        "    heartbeat('Skipping F2 (FFD~entropy) — required columns missing.')"
      ]
    },
# EEG per-subject plot (F3)
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Figure F3 — EEG vs KEC (per-subject trends)\n",
        "For each subject, plot EEG power vs a KEC metric and fit a simple line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "def per_subject_trends(df, eeg_col='ThetaPower', xvar='entropy', savepath=None):\n",
        "    need = {'Subject', eeg_col, xvar}\n",
        "    if not need.issubset(df.columns):\n",
        "        heartbeat(f\"Missing columns for F3: {need}\")\n",
        "        return\n",
        "    subs = sorted(df['Subject'].unique())\n",
        "    nsub = len(subs)\n",
        "    if nsub==0:\n",
        "        heartbeat('No subjects found for F3.')\n",
        "        return\n",
        "    ncols = 4\n",
        "    nrows = int(math.ceil(nsub / ncols))\n",
        "    plt.figure(figsize=(12, 3*nrows))\n",
        "    for i, s in enumerate(subs, 1):\n",
        "        ax = plt.subplot(nrows, ncols, i)\n",
        "        sdf = df[df['Subject']==s].dropna(subset=[eeg_col, xvar])\n",
        "        if len(sdf)<5:\n",
        "            ax.set_title(f\"{s} (insufficient data)\")\n",
        "            continue\n",
        "        ax.scatter(sdf[xvar], sdf[eeg_col], alpha=0.3)\n",
        "        try:\n",
        "            m = smf.ols(f\"{eeg_col} ~ {xvar}\", data=sdf).fit()\n",
        "            xs = np.linspace(sdf[xvar].min(), sdf[xvar].max(), 50)\n",
        "            ys = m.params['Intercept'] + m.params[xvar]*xs\n",
        "            ax.plot(xs, ys)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        ax.set_title(str(s))\n",
        "        ax.set_xlabel(xvar)\n",
        "        ax.set_ylabel(eeg_col)\n",
        "    plt.tight_layout()\n",
        "    if savepath:\n",
        "        plt.savefig(savepath, dpi=150)\n",
        "        heartbeat(f\"Saved figure → {savepath}\")\n",
        "    plt.show()\n",
        "\n",
        "if len(df)>0 and {'Subject','entropy'}.issubset(df.columns) and any(c in df.columns for c in ['ThetaPower','AlphaPower']):\n",
        "    eegc = 'ThetaPower' if 'ThetaPower' in df.columns else 'AlphaPower'\n",
        "    per_subject_trends(df, eeg_col=eegc, xvar='entropy', savepath=FIG_DIR/'F3_EEG_vs_KEC.png')\n",
        "else:\n",
        "    heartbeat('Skipping F3 — EEG columns or entropy missing.')"
      ]
    },
# Bootstrap skeleton
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) (Optional) Bootstrap CIs (by subject)\n",
        "Bootstrap model coefficients with resampling clustered at subject level for robust CIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "def bootstrap_coefs(df, formula, group_col='Subject', B=200, seed=SEED):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    groups = df[group_col].unique()\n",
        "    coefs = []\n",
        "    for b in range(B):\n",
        "        # resample groups with replacement, then take all rows from selected groups\n",
        "        samp_groups = rng.choice(groups, size=len(groups), replace=True)\n",
        "        samp_df = pd.concat([df[df[group_col]==g] for g in samp_groups], ignore_index=True)\n",
        "        try:\n",
        "            m = smf.ols(formula, data=samp_df).fit()\n",
        "            coefs.append(m.params)\n",
        "        except Exception:\n",
        "            continue\n",
        "    if not coefs:\n",
        "        return pd.DataFrame()\n",
        "    coef_df = pd.DataFrame(coefs)\n",
        "    return coef_df\n",
        "\n",
        "# Example: bootstrap only if small enough\n",
        "if len(df)>0 and {'FFD','entropy','length','log_freq','Subject'}.issubset(df.columns):\n",
        "    formula = 'FFD ~ entropy + length + log_freq'\n",
        "    heartbeat('Bootstrapping OLS (clustered by Subject) for FFD ~ entropy + length + log_freq (B=200)')\n",
        "    boot = bootstrap_coefs(df[['FFD','entropy','length','log_freq','Subject']].dropna(), formula, B=200)\n",
        "    if len(boot)>0:\n",
        "        boot.to_csv(PROC_DIR/'boot_ols_ffd_entropy.csv', index=False)\n",
        "        heartbeat(f\"Saved bootstrap coefs → {PROC_DIR/'boot_ols_ffd_entropy.csv'}\")\n",
        "else:\n",
        "    heartbeat('Skipping bootstrap — required columns missing.')"
      ]
    },
# QA summary
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) QA Summary\n",
        "Emit a small JSON with info about fitted models and generated artifacts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count":None,
      "source": [
        "qa = {\n",
        "  'timestamp_utc': '"+now+"',\n",
        "  'n_rows_merged': int(len(merged)) if len(merged)>0 else 0,\n",
        "  'et_cols': ET_COLS,\n",
        "  'eeg_cols': EEG_COLS,\n",
        "  'kec_cols': KEC_COLS,\n",
        "  'reading_coeffs_csv': str(PROC_DIR/'models_reading_coeffs.csv') if (PROC_DIR/'models_reading_coeffs.csv').exists() else None,\n",
        "  'boot_ffd_entropy': str(PROC_DIR/'boot_ols_ffd_entropy.csv') if (PROC_DIR/'boot_ols_ffd_entropy.csv').exists() else None,\n",
        "  'fig_F2': str(FIG_DIR/'F2_reading_vs_KEC.png') if (FIG_DIR/'F2_reading_vs_KEC.png').exists() else None,\n",
        "  'fig_F3': str(FIG_DIR/'F3_EEG_vs_KEC.png') if (FIG_DIR/'F3_EEG_vs_KEC.png').exists() else None\n",
        "}\n",
        "with open(REPORTS_DIR/'qa_kec_models.json', 'w') as f:\n",
        "    json.dump(qa, f, indent=2)\n",
        "heartbeat(f\"Saved QA JSON → {REPORTS_DIR/'qa_kec_models.json'}\")"
      ]
    },
# Final notes
  {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- For full analyses, extend models to GD, TRT (on `log_TRT`), GPT (on `log_GPT`), and EEG bands.\n",
        "- Consider MixedLM with random intercepts for Subject and SentenceID if computationally feasible.\n",
        "- Use `cov_type='cluster'` with appropriate grouping for robust SEs in OLS.\n",
        "- Keep figures under `figures/metrics/` and tables under `data/processed/` to match repo standards.\n",
        "- Seeds fixed for reproducibility; adjust `B` for bootstrap to balance CI quality and runtime.\n"
      ]
    }
  ]
}

nb_path = os.path.join(base_notebooks,
"04_zuco_models.ipynb")
with open(nb_path,
"w") as f:
    json.dump(nb, f)

nb_path
