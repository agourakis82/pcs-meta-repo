\section*{Introduction}
Human language can be represented as a complex network of associations, which we quantify using a novel multi-faceted metric. We leverage the \textbf{Small World of Words (SWOW)} datasets – large-scale word association norms collected from thousands of participants – to construct semantic graphs for multiple languages [oai_citation:0‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=SWOW) [oai_citation:1‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=SWOW). In these graphs, words (nodes) are linked if a participant produced one in response to the other (directed weighted edges). Such crowdsourced association data (e.g. 12k cue words in English [oai_citation:2‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=SWOW), and similar datasets in Spanish [oai_citation:3‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=SWOW) and other languages) provide an empirically grounded structure of the mental lexicon, covering both strong common associations and weaker peripheral ones. We hypothesize that the \textit{structure} of these semantic neighborhoods – in terms of uncertainty, connectivity, and community coherence – has measurable effects on cognitive processing during reading.

To capture this structure, we define the **Knowledge Entropy Curvature (KEC)** metric for each word in the semantic network. \textbf{KEC} comprises three components: (i) \textit{transition entropy}, measuring the unpredictability of a word’s associations (higher entropy means the word leads to many diverse, equally likely ideas); (ii) \textit{local curvature}, measuring how tightly connected or bridge-like the word is in the graph’s geometry (using discrete Ricci curvature formulations) [oai_citation:4‡en.wikipedia.org](https://en.wikipedia.org/wiki/Ricci_curvature#:~:text=Notions%20of%20Ricci%20curvature%20on,11) [oai_citation:5‡en.wikipedia.org](https://en.wikipedia.org/wiki/Ricci_curvature#:~:text=8.%20,Forman%202003); and (iii) \textit{meso-scale coherence}, quantifying how well the word’s neighborhood aligns with community structure (e.g. if its associates cluster within the same semantic community or spread across topics). For curvature, we adapt two canonical definitions from network theory: Ollivier’s Ricci curvature (a transport-based measure) [oai_citation:6‡en.wikipedia.org](https://en.wikipedia.org/wiki/Ricci_curvature#:~:text=8.%20,Forman%202003) [oai_citation:7‡en.wikipedia.org](https://en.wikipedia.org/wiki/Ricci_curvature#:~:text=%2A%20Ollivier%2C%20Yann%20%282009,of%20Mathematics%20%2C%20%20235) and Forman’s combinatorial curvature [oai_citation:8‡en.wikipedia.org](https://en.wikipedia.org/wiki/Ricci_curvature#:~:text=4.%20,189) [oai_citation:9‡en.wikipedia.org](https://en.wikipedia.org/wiki/Ricci_curvature#:~:text=,3), which offer complementary perspectives on local graph connectivity. By calculating these metrics on the SWOW graph, with appropriate normalization and smoothing, we obtain a reproducible KEC profile for each word. High KEC values might indicate words that are semantically surprising (high entropy), act as local bridges (low curvature indicating a “flat” connection to disparate regions), or sit at crossroads of multiple semantic communities (low coherence).

We then ask whether these network-derived properties correlate with **neurocognitive reading difficulty**. For this, we use the \textbf{Zurich Cognitive Language Processing Corpus (ZuCo)} [oai_citation:10‡nature.com](https://www.nature.com/articles/sdata2018291?error=cookies_not_supported&code=c66dd047-201a-406b-ba82-df650f936d48#:~:text=How%20to%20cite%20this%20article%3A,291%20%282018), which provides concurrent eye-tracking and EEG recordings from adults reading natural sentences [oai_citation:11‡nature.com](https://www.nature.com/articles/sdata2018291?error=cookies_not_supported&code=c66dd047-201a-406b-ba82-df650f936d48#:~:text=We%20present%20the%20Zurich%20Cognitive,various%20tasks%2C%20in%20particular%20for) [oai_citation:12‡nature.com](https://www.nature.com/articles/sdata2018291?error=cookies_not_supported&code=c66dd047-201a-406b-ba82-df650f936d48#:~:text=reading%20natural%20sentences,entity%20and%20relation%20extraction%20and). ZuCo 1.0 (and the expanded 2.0) encompass over a thousand English sentences where each word’s reading is annotated with eye movement measures and neural activity [oai_citation:13‡nature.com](https://www.nature.com/articles/sdata2018291?error=cookies_not_supported&code=c66dd047-201a-406b-ba82-df650f936d48#:~:text=We%20present%20the%20Zurich%20Cognitive,signals%20lend%20themselves%20to%20train) [oai_citation:14‡aclanthology.org](https://aclanthology.org/2020.lrec-1.18.pdf#:~:text=We%20recorded%20and%20preprocessed%20ZuCo,The%20data%20is%20freely). Key eye-tracking metrics include First Fixation Duration (FFD), Gaze Duration (GD), Total Reading Time (TRT), and Go-Past Time (GPT), which index the immediate and cumulative processing time on a word [oai_citation:15‡aclanthology.org](https://aclanthology.org/2020.lrec-1.18.pdf#:~:text=features%3A%20,progressing%20to%20the%20right%20of). For EEG, we focus on band-limited spectral power (e.g. theta, alpha bands) around each word’s fixation, as these reflect cognitive load and linguistic processing at the neural level [oai_citation:16‡arxiv.org](https://arxiv.org/html/2208.06348v5#:~:text=The%20ZuCo%20Dataset%20Hollenstein%20et,tasks%20during%20natural%20reading). The ZuCo dataset is well-suited for our analysis as it provides word-level alignment of text and brain signals in natural reading [oai_citation:17‡aclanthology.org](https://aclanthology.org/2020.lrec-1.18.pdf#:~:text=We%20recorded%20and%20preprocessed%20ZuCo,The%20data%20is%20freely). We map each word occurrence in the ZuCo sentences to its KEC metrics (derived from the SWOW-based English semantic network). This results in a combined dataset of word-level observations with predictors from the semantic graph (entropy, curvature, coherence) and outcomes from human reading behavior (eye movement durations, EEG responses).

\section*{Methods}
\textbf{Data Preparation:} We obtained SWOW raw association data for English (SWOW-EN) and other languages from their official releases [oai_citation:18‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=SWOW) [oai_citation:19‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=Updated%2018%20October%202018), ensuring consistency by using the published preprocessed sets (each cue word has responses from a fixed number of participants). Graphs were constructed using directed edges weighted by response frequency, as described above. All raw data sources are public and licensed for research use, with no personal identifiers (e.g., SWOW is open-access [oai_citation:20‡smallworldofwords.org](https://smallworldofwords.org/en/project/research#:~:text=Many%20hours%20of%20work%20have,the%20link%20to%20the%20study) and ZuCo is shared under appropriate agreements [oai_citation:21‡nature.com](https://www.nature.com/articles/sdata2018291?error=cookies_not_supported&code=c66dd047-201a-406b-ba82-df650f936d48#:~:text=We%20present%20the%20Zurich%20Cognitive,signals%20lend%20themselves%20to%20train)). For ZuCo, we used version 1.0 and 2.0 corpora [oai_citation:22‡aclanthology.org](https://aclanthology.org/2020.lrec-1.18.pdf#:~:text=We%20recorded%20and%20preprocessed%20ZuCo,The%20data%20is%20freely), parsing eye-tracking logs and EEG signal files to extract the relevant measures per word. Fixation events shorter than 100 ms were excluded as standard in eye-movement research (to avoid spurious fixations [oai_citation:23‡aclanthology.org](https://aclanthology.org/2020.lrec-1.18.pdf#:~:text=of%2050%20pixels%20to%20the,iii)). EEG signals were preprocessed as in the original corpus publication (e.g. using Automagic pipeline) [oai_citation:24‡aclanthology.org](https://aclanthology.org/2020.lrec-1.18.pdf#:~:text=that%20originated%20from%20the%20current,e), then segmented per word using the eye-tracking alignment (from fixation onset to word gaze end). We then computed band-power features (averaged within the fixation window for delta, theta, alpha, etc. frequencies). All processed data were saved in structured tables (ensuring reproducibility via fixed random seeds for any resampling and logging data versions and SHA-256 hashes in a `provenance.yaml` file).

\textbf{Statistical Analysis:} We examined the relationship between KEC metrics and reading measures using mixed-effects regression models. Specifically, for each eye-tracking metric (FFD, GD, TRT, GPT) and each EEG feature, we fit linear mixed models with KEC entropy, curvature, and coherence as main predictors. To control for known confounds, we included lexical covariates: word length (in characters) and lexical frequency (log-transformed) [oai_citation:25‡link.springer.com](https://link.springer.com/content/pdf/10.3758/BF03206448.pdf#:~:text=The%20effect%20of%20word%20predictability,and%20word), since frequent or short words are generally read faster [oai_citation:26‡link.springer.com](https://link.springer.com/content/pdf/10.3758/BF03206448.pdf#:~:text=The%20effect%20of%20word%20predictability,and%20word). We also added random intercepts for each participant and each sentence to account for repeated measures and individual/text differences (a hierarchical model structure). This approach captures the within-subject variance and any sentence-level effects, improving the robustness of our estimates [oai_citation:27‡cameron.econ.ucdavis.edu](https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf#:~:text=panel%20data,unusual%20to%20have%20applications%20where) [oai_citation:28‡cameron.econ.ucdavis.edu](https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf#:~:text=ignore%20such%20correlation,regressor%2C%20that%20takes%20the%20same). For hypothesis testing on fixed effects (KEC terms), we used cluster-robust standard errors clustered by subject [oai_citation:29‡cameron.econ.ucdavis.edu](https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf#:~:text=Failure%20to%20control%20for%20within,for%20such%20control%20increases%20not), given the relatively small number of subjects ($n=12$) and the potential for residual correlations within individuals.

Significance of effects was assessed at $\alpha=0.05$ with correction for multiple comparisons across the multiple outcome measures. Rather than a strict Bonferroni correction (which would be overly conservative given correlated measures), we controlled the False Discovery Rate using the Benjamini–Hochberg procedure [oai_citation:30‡en.wikipedia.org](https://en.wikipedia.org/wiki/False_discovery_rate#:~:text=The%20FDR%20concept%20was%20formally,cited%20statistical%20papers.%5B%205). This ensures that the expected proportion of false positives among significant findings is below 5%. We report effect sizes as regression coefficients with 95% confidence intervals obtained via nonparametric bootstrap resampling (1,000 iterations) [oai_citation:31‡ssc.wisc.edu](https://www.ssc.wisc.edu/~xshi/econ715/Lecture_10_bootstrap.pdf#:~:text=5,1993), which provides robust uncertainty estimates even if residuals deviate from normality. To facilitate interpretation, we also produced partial dependence plots (Figure 2 and Figure 3) illustrating how a change in a single KEC component (holding others constant) is associated with changes in reading time or EEG response. All analyses were conducted in Python (pandas, statsmodels, and networkx libraries) with a fixed random seed for any stochastic elements to ensure reproducibility [oai_citation:32‡GitHub](https://github.com/agourakis82/pcs-meta-repo/blob/ea8b001102fe540af7a530a6e93f89afa4c204e1/instrucoes_projeto_v4.3.md#L16-L19) [oai_citation:33‡GitHub](https://github.com/agourakis82/pcs-meta-repo/blob/ea8b001102fe540af7a530a6e93f89afa4c204e1/instrucoes_projeto_v4.3.md#L40-L48).

\textbf{Theoretical Interpretation:} Our working theory is that words which are \textit{harder to predict or integrate} in the semantic context impose greater cognitive load, manifesting as longer fixations and enhanced neural responses. KEC provides a quantitative handle on this “symbolic surprisal.” For example, a word with high entropy (scattering associations across many concepts) might be less expected in a given context, akin to high surprisal in language models [oai_citation:34‡aclanthology.org](https://aclanthology.org/2020.conll-1.53.pdf#:~:text=potential%20is%20generally%20understood%20to,research%20on%20the%20topic%2C%20many), and thus elicit a larger N400 component in EEG (reflecting greater semantic integration difficulty) [oai_citation:35‡aclanthology.org](https://aclanthology.org/2020.conll-1.53.pdf#:~:text=potential%20is%20generally%20understood%20to,research%20on%20the%20topic%2C%20many). Likewise, a word bridging disparate semantic communities (low curvature, low coherence) might require additional processing, as it may introduce an unexpected thematic shift or ambiguity. This aligns with past findings that words which are semantically incongruent or weakly related to context cause both prolonged gaze durations and amplified brain responses (e.g., N400 amplitude) [oai_citation:36‡aclanthology.org](https://aclanthology.org/2020.conll-1.53.pdf#:~:text=potential%20is%20generally%20understood%20to,research%20on%20the%20topic%2C%20many) [oai_citation:37‡link.springer.com](https://link.springer.com/content/pdf/10.3758/BF03206448.pdf#:~:text=The%20effect%20of%20word%20predictability,and%20word). By linking a network-theoretic measure (curvature in the semantic graph) to cognitive signals, we aim to provide new evidence that the \textit{geometric structure of semantic knowledge} influences real-time language processing in the brain.
